{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f871a58d",
   "metadata": {},
   "source": [
    "## 0. Infrastructure Setup\n",
    "\n",
    "### 0.1 Utils Module\n",
    "All helpful methods including validate local path, local logging, serialise and deserialise json file, read and write files, create and delete path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "rptw3m85tba",
   "metadata": {},
   "source": [
    "# Minecraft Voxel World LLM Training\n",
    "\n",
    "## Project Overview\n",
    "This notebook implements a complete pipeline for training Large Language Models (LLMs) on Minecraft voxel-based sequential prediction tasks:\n",
    "- **Frame Prediction**: Given current voxel state + action → predict next voxel state\n",
    "- **Action Recognition**: Given current voxel state + next voxel state → predict action taken\n",
    "\n",
    "## Dataset\n",
    "- **Source**: Minecraft gameplay data with 3D voxel representations\n",
    "- **Format**: Sequential .npy files containing voxel grids and actions\n",
    "- **Structure**: Each frame contains a 3×3×3 block grid and action vector\n",
    "\n",
    "## Models\n",
    "- **Qwen 3 0.6B**: Small-scale model for efficient training\n",
    "- **Qwen 3 4B**: Larger model for improved performance\n",
    "\n",
    "## Methods\n",
    "- In-context learning (few-shot prompting with training examples)\n",
    "- Supervised fine-tuning with LoRA for frame reconstruction\n",
    "- Supervised fine-tuning with LoRA for action recognition\n",
    "\n",
    "---\n",
    "\n",
    "## Table of Contents\n",
    "\n",
    "### 0. Infrastructure Setup\n",
    "- **0.1** Utils Module - File I/O, logging, JSON operations\n",
    "- **0.2** Model Wrapper Class - Training, evaluation, checkpoint management\n",
    "- **0.3** Plot Evaluation Class - Conference-quality visualizations\n",
    "- **0.4** Hyperparameter Configuration - Grid search support\n",
    "\n",
    "### 1. Setup\n",
    "- **1.1** Load Models - Qwen 3 0.6B and 4B configuration\n",
    "- **1.2** Load Minecraft Dataset - Sequential voxel frames with actions\n",
    "- **1.3** Split Data - Train/val/test split (70%/15%/15%)\n",
    "\n",
    "### 2. In-Context Learning Evaluation\n",
    "- **2.1** Frame Reconstruction - Input: x+y, Output: z (with 3 training examples as context)\n",
    "- **2.2** Frame Reconstruction Plots - Visualization of results\n",
    "- **2.3** Action Recognition - Input: x+z, Output: y (with 3 training examples as context)\n",
    "- **2.4** Action Recognition Plots - Visualization of results\n",
    "\n",
    "### 3. Supervised Fine-Tuning (LoRA) for Frame Reconstruction \n",
    "- **3.1** Fine-tune Frame Reconstruction - LoRA adaptation with W&B monitoring\n",
    "- **3.2** Evaluate Fine-tuned Models - Test set performance\n",
    "- **3.3** Plot Fine-tuning Results - Compare in-context vs fine-tuned\n",
    "\n",
    "### 4. Supervised Fine-Tuning (LoRA) for Action Recognition\n",
    "- **4.1** Fine-tune Action Recognition - Train LoRA adapters to predict discrete actions\n",
    "- **4.2** Evaluate Action Recognition - Test set metrics and JSON export\n",
    "- **4.3** Plot Action Recognition Comparison - Bar charts versus zero-shot baseline\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5d1533a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Utils module loaded successfully\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import logging\n",
    "import os\n",
    "from pathlib import Path\n",
    "from typing import Any, Union\n",
    "from datetime import datetime\n",
    "\n",
    "class Utils:\n",
    "    \"\"\"Utility class for file operations, logging, and path management.\"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def validate_path(path: Union[str, Path], create: bool = False) -> Path:\n",
    "        \"\"\"Validate and optionally create a path.\"\"\"\n",
    "        path = Path(path)\n",
    "        if create:\n",
    "            path.mkdir(parents=True, exist_ok=True)\n",
    "        return path\n",
    "    \n",
    "    @staticmethod\n",
    "    def setup_logging(log_dir: Union[str, Path], name: str = \"experiment\") -> logging.Logger:\n",
    "        \"\"\"Setup logging to file and console.\"\"\"\n",
    "        log_dir = Utils.validate_path(log_dir, create=True)\n",
    "        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "        log_file = log_dir / f\"{name}_{timestamp}.log\"\n",
    "        \n",
    "        logger = logging.getLogger(name)\n",
    "        logger.setLevel(logging.INFO)\n",
    "        \n",
    "        # Clear existing handlers\n",
    "        logger.handlers.clear()\n",
    "        \n",
    "        # File handler\n",
    "        fh = logging.FileHandler(log_file)\n",
    "        fh.setLevel(logging.INFO)\n",
    "        \n",
    "        # Console handler\n",
    "        ch = logging.StreamHandler()\n",
    "        ch.setLevel(logging.INFO)\n",
    "        \n",
    "        # Formatter\n",
    "        formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
    "        fh.setFormatter(formatter)\n",
    "        ch.setFormatter(formatter)\n",
    "        \n",
    "        logger.addHandler(fh)\n",
    "        logger.addHandler(ch)\n",
    "        \n",
    "        return logger\n",
    "    \n",
    "    @staticmethod\n",
    "    def save_json(data: Any, path: Union[str, Path]) -> None:\n",
    "        \"\"\"Save data to JSON file.\"\"\"\n",
    "        path = Path(path)\n",
    "        path.parent.mkdir(parents=True, exist_ok=True)\n",
    "        with open(path, 'w') as f:\n",
    "            json.dump(data, f, indent=2)\n",
    "    \n",
    "    @staticmethod\n",
    "    def load_json(path: Union[str, Path]) -> Any:\n",
    "        \"\"\"Load data from JSON file.\"\"\"\n",
    "        path = Path(path)\n",
    "        if not path.exists():\n",
    "            return None\n",
    "        with open(path, 'r') as f:\n",
    "            return json.load(f)\n",
    "    \n",
    "    @staticmethod\n",
    "    def read_file(path: Union[str, Path]) -> str:\n",
    "        \"\"\"Read text file.\"\"\"\n",
    "        with open(path, 'r') as f:\n",
    "            return f.read()\n",
    "    \n",
    "    @staticmethod\n",
    "    def write_file(path: Union[str, Path], content: str) -> None:\n",
    "        \"\"\"Write text file.\"\"\"\n",
    "        path = Path(path)\n",
    "        path.parent.mkdir(parents=True, exist_ok=True)\n",
    "        with open(path, 'w') as f:\n",
    "            f.write(content)\n",
    "    \n",
    "    @staticmethod\n",
    "    def delete_path(path: Union[str, Path]) -> None:\n",
    "        \"\"\"Delete file or directory.\"\"\"\n",
    "        path = Path(path)\n",
    "        if path.is_file():\n",
    "            path.unlink()\n",
    "        elif path.is_dir():\n",
    "            import shutil\n",
    "            shutil.rmtree(path)\n",
    "\n",
    "print(\"Utils module loaded successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b16dbc7c",
   "metadata": {},
   "source": [
    "### 0.2 Model Wrapper Class\n",
    "Including loading with name method, train with dataloaders method, and evaluate method.\n",
    "\n",
    "With loaded data train and stop in val and monitor via W&B. Do not pass model parameters to W&B. Keep them in local dir `checkpoints/` with proper naming and also keep a log in the dir `logs/`.  Create a JSON file with proper name of task in the working dir given the match between the run folder path under checkpoints and the run log path.\n",
    "\n",
    "The checkpoint resume from latest feature should be implemented - we do not want to train repeatedly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cf7438fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/shirox/workspace/nlp701-lab/project/llm/.venv/lib/python3.11/site-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'repr' attribute with value False was provided to the `Field()` function, which has no effect in the context it was used. 'repr' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.\n",
      "  warnings.warn(\n",
      "/home/shirox/workspace/nlp701-lab/project/llm/.venv/lib/python3.11/site-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'frozen' attribute with value True was provided to the `Field()` function, which has no effect in the context it was used. 'frozen' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ModelWrapper class loaded successfully\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, get_linear_schedule_with_warmup\n",
    "from peft import LoraConfig, get_peft_model, PeftModel\n",
    "import wandb\n",
    "from tqdm import tqdm\n",
    "from datetime import datetime\n",
    "import glob\n",
    "from pathlib import Path\n",
    "\n",
    "class ModelWrapper:\n",
    "    \"\"\"Wrapper class for loading, training, and evaluating LLM models.\"\"\"\n",
    "\n",
    "    def __init__(self, model_name: str, device: str = \"cuda\" if torch.cuda.is_available() else \"cpu\"):\n",
    "        \"\"\"Initialize model wrapper.\"\"\"\n",
    "        self.model_name = model_name\n",
    "        self.device = device\n",
    "        self.model = None\n",
    "        self.tokenizer = None\n",
    "        self.is_lora = False\n",
    "        self.checkpoint_dir = None\n",
    "        self.log_dir = None\n",
    "\n",
    "    def load_model(self, use_lora: bool = False, lora_config: dict = None):\n",
    "        \"\"\"Load model and tokenizer.\"\"\"\n",
    "        print(f\"Loading model: {self.model_name}\")\n",
    "\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(self.model_name)\n",
    "        if self.tokenizer.pad_token is None:\n",
    "            self.tokenizer.pad_token = self.tokenizer.eos_token\n",
    "\n",
    "        self.model = AutoModelForCausalLM.from_pretrained(\n",
    "            self.model_name,\n",
    "            torch_dtype=torch.float16 if self.device == \"cuda\" else torch.float32,\n",
    "            device_map=\"auto\" if self.device == \"cuda\" else None\n",
    "        )\n",
    "\n",
    "        if use_lora:\n",
    "            default_config = {\n",
    "                \"r\": 8,\n",
    "                \"lora_alpha\": 32,\n",
    "                \"target_modules\": [\"q_proj\", \"v_proj\"],\n",
    "                \"lora_dropout\": 0.1,\n",
    "                \"bias\": \"none\",\n",
    "                \"task_type\": \"CAUSAL_LM\"\n",
    "            }\n",
    "            if lora_config:\n",
    "                default_config.update(lora_config)\n",
    "\n",
    "            lora_config_obj = LoraConfig(**default_config)\n",
    "            self.model = get_peft_model(self.model, lora_config_obj)\n",
    "            self.is_lora = True\n",
    "            print(\"LoRA adapter applied\")\n",
    "            print(\"NOTE: Only LoRA adapter weights will be saved (saves disk space)\")\n",
    "\n",
    "        if self.device != \"cuda\":\n",
    "            self.model = self.model.to(self.device)\n",
    "\n",
    "        print(f\"Model loaded on {self.device}\")\n",
    "        return self\n",
    "\n",
    "    def setup_training(self, task_name: str, run_name: str = None):\n",
    "        \"\"\"Setup directories and logging for training.\"\"\"\n",
    "        if run_name is None:\n",
    "            run_name = f\"{task_name}_{self.model_name.split('/')[-1]}_{datetime.now().strftime('%Y%m%d_%H%M%S')}\"\n",
    "\n",
    "        self.checkpoint_dir = Utils.validate_path(f\"checkpoints/{run_name}\", create=True)\n",
    "        self.log_dir = Utils.validate_path(f\"logs/{run_name}\", create=True)\n",
    "\n",
    "        metadata = {\n",
    "            \"task_name\": task_name,\n",
    "            \"run_name\": run_name,\n",
    "            \"model_name\": self.model_name,\n",
    "            \"checkpoint_dir\": str(self.checkpoint_dir),\n",
    "            \"log_dir\": str(self.log_dir),\n",
    "            \"is_lora\": self.is_lora,\n",
    "            \"created_at\": datetime.now().isoformat()\n",
    "        }\n",
    "\n",
    "        return run_name, metadata\n",
    "\n",
    "    def find_latest_checkpoint(self):\n",
    "        \"\"\"Find latest checkpoint for resuming training.\"\"\"\n",
    "        if not self.checkpoint_dir or not Path(self.checkpoint_dir).exists():\n",
    "            return None\n",
    "\n",
    "        checkpoints = glob.glob(str(Path(self.checkpoint_dir) / \"checkpoint_epoch_*.pt\"))\n",
    "        if not checkpoints:\n",
    "            return None\n",
    "\n",
    "        epochs = [int(Path(c).stem.split('_')[-1]) for c in checkpoints]\n",
    "        latest_epoch = max(epochs)\n",
    "        return Path(self.checkpoint_dir) / f\"checkpoint_epoch_{latest_epoch}.pt\"\n",
    "\n",
    "    def train(self, train_loader: DataLoader, val_loader: DataLoader,\n",
    "              config: dict, task_name: str, use_wandb: bool = True):\n",
    "        \"\"\"Train model with LoRA - saves only adapter weights to save disk space.\"\"\"\n",
    "        run_name, metadata = self.setup_training(task_name)\n",
    "        logger = Utils.setup_logging(self.log_dir, task_name)\n",
    "\n",
    "        latest_checkpoint = self.find_latest_checkpoint()\n",
    "        start_epoch = 0\n",
    "\n",
    "        if latest_checkpoint:\n",
    "            print(f\"Found checkpoint: {latest_checkpoint}\")\n",
    "            checkpoint = torch.load(latest_checkpoint)\n",
    "            self.model.load_state_dict(checkpoint['model_state_dict'])\n",
    "            start_epoch = checkpoint['epoch'] + 1\n",
    "            print(f\"Resuming from epoch {start_epoch}\")\n",
    "\n",
    "        if use_wandb:\n",
    "            wandb.init(\n",
    "                project=config.get('wandb_project', 'minecraft-llm'),\n",
    "                name=run_name,\n",
    "                config={k: v for k, v in config.items() if not k.startswith('wandb')}\n",
    "            )\n",
    "\n",
    "        optimizer = torch.optim.AdamW(self.model.parameters(), lr=config['learning_rate'])\n",
    "        num_training_steps = len(train_loader) * config['num_epochs']\n",
    "        scheduler = get_linear_schedule_with_warmup(\n",
    "            optimizer,\n",
    "            num_warmup_steps=config.get('warmup_steps', 0),\n",
    "            num_training_steps=num_training_steps\n",
    "        )\n",
    "\n",
    "        best_val_loss = float('inf')\n",
    "\n",
    "        for epoch in range(start_epoch, config['num_epochs']):\n",
    "            self.model.train()\n",
    "            train_loss = 0\n",
    "\n",
    "            pbar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{config['num_epochs']}\")\n",
    "            for batch in pbar:\n",
    "                input_ids = batch['input_ids'].to(self.device)\n",
    "                attention_mask = batch['attention_mask'].to(self.device)\n",
    "                labels = batch['labels'].to(self.device)\n",
    "\n",
    "                outputs = self.model(\n",
    "                    input_ids=input_ids,\n",
    "                    attention_mask=attention_mask,\n",
    "                    labels=labels\n",
    "                )\n",
    "\n",
    "                loss = outputs.loss\n",
    "                loss.backward()\n",
    "\n",
    "                torch.nn.utils.clip_grad_norm_(self.model.parameters(), config.get('max_grad_norm', 1.0))\n",
    "\n",
    "                optimizer.step()\n",
    "                scheduler.step()\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                train_loss += loss.item()\n",
    "                pbar.set_postfix({'loss': loss.item()})\n",
    "\n",
    "                if use_wandb:\n",
    "                    wandb.log({'train_loss_step': loss.item()})\n",
    "\n",
    "            avg_train_loss = train_loss / len(train_loader)\n",
    "            val_loss = self.evaluate_loss(val_loader)\n",
    "\n",
    "            logger.info(f\"Epoch {epoch+1}: train_loss={avg_train_loss:.4f}, val_loss={val_loss:.4f}\")\n",
    "\n",
    "            if use_wandb:\n",
    "                wandb.log({\n",
    "                    'epoch': epoch + 1,\n",
    "                    'train_loss': avg_train_loss,\n",
    "                    'val_loss': val_loss\n",
    "                })\n",
    "\n",
    "            try:\n",
    "                if self.is_lora:\n",
    "                    adapter_path = Path(self.checkpoint_dir) / f\"adapter_epoch_{epoch}\"\n",
    "                    self.model.save_pretrained(adapter_path)\n",
    "                    state_path = Path(self.checkpoint_dir) / f\"state_epoch_{epoch}.pt\"\n",
    "                    torch.save({\n",
    "                        'epoch': epoch,\n",
    "                        'optimizer_state_dict': optimizer.state_dict(),\n",
    "                        'train_loss': avg_train_loss,\n",
    "                        'val_loss': val_loss\n",
    "                    }, state_path)\n",
    "                else:\n",
    "                    checkpoint_path = Path(self.checkpoint_dir) / f\"checkpoint_epoch_{epoch}.pt\"\n",
    "                    torch.save({\n",
    "                        'epoch': epoch,\n",
    "                        'model_state_dict': self.model.state_dict(),\n",
    "                        'optimizer_state_dict': optimizer.state_dict(),\n",
    "                        'train_loss': avg_train_loss,\n",
    "                        'val_loss': val_loss\n",
    "                    }, checkpoint_path)\n",
    "\n",
    "                if val_loss < best_val_loss:\n",
    "                    best_val_loss = val_loss\n",
    "                    if self.is_lora:\n",
    "                        best_adapter_path = Path(self.checkpoint_dir) / \"best_lora_adapter\"\n",
    "                        self.model.save_pretrained(best_adapter_path)\n",
    "                        logger.info(f\"Best LoRA adapter saved with val_loss={val_loss:.4f}\")\n",
    "                    else:\n",
    "                        best_model_path = Path(self.checkpoint_dir) / \"best_model.pt\"\n",
    "                        torch.save(self.model.state_dict(), best_model_path)\n",
    "                        logger.info(f\"Best model saved with val_loss={val_loss:.4f}\")\n",
    "\n",
    "            except RuntimeError as e:\n",
    "                logger.error(f\"Failed to save checkpoint: {e}\")\n",
    "                print(f\"WARNING: Could not save checkpoint due to disk space. Error: {e}\")\n",
    "\n",
    "        if use_wandb:\n",
    "            wandb.finish()\n",
    "\n",
    "        metadata['training_completed'] = datetime.now().isoformat()\n",
    "        metadata['best_val_loss'] = best_val_loss\n",
    "\n",
    "        return metadata\n",
    "\n",
    "    def evaluate_loss(self, dataloader: DataLoader):\n",
    "        \"\"\"Evaluate loss on a dataloader.\"\"\"\n",
    "        self.model.eval()\n",
    "        total_loss = 0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for batch in dataloader:\n",
    "                input_ids = batch['input_ids'].to(self.device)\n",
    "                attention_mask = batch['attention_mask'].to(self.device)\n",
    "                labels = batch['labels'].to(self.device)\n",
    "\n",
    "                outputs = self.model(\n",
    "                    input_ids=input_ids,\n",
    "                    attention_mask=attention_mask,\n",
    "                    labels=labels\n",
    "                )\n",
    "\n",
    "                total_loss += outputs.loss.item()\n",
    "\n",
    "        return total_loss / len(dataloader)\n",
    "\n",
    "    def evaluate(self, test_loader: DataLoader, max_new_tokens: int = 256):\n",
    "        \"\"\"Evaluate model and return predictions.\"\"\"\n",
    "        self.model.eval()\n",
    "        predictions = []\n",
    "        targets = []\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for batch in tqdm(test_loader, desc=\"Evaluating\"):\n",
    "                input_ids = batch['input_ids'].to(self.device)\n",
    "                attention_mask = batch['attention_mask'].to(self.device)\n",
    "\n",
    "                outputs = self.model.generate(\n",
    "                    input_ids=input_ids,\n",
    "                    attention_mask=attention_mask,\n",
    "                    max_new_tokens=max_new_tokens,\n",
    "                    pad_token_id=self.tokenizer.pad_token_id,\n",
    "                    do_sample=False\n",
    "                )\n",
    "\n",
    "                for i, output in enumerate(outputs):\n",
    "                    input_len = input_ids[i].shape[0]\n",
    "                    generated = output[input_len:]\n",
    "                    pred_text = self.tokenizer.decode(generated, skip_special_tokens=True)\n",
    "                    predictions.append(pred_text)\n",
    "\n",
    "                if 'target_text' in batch:\n",
    "                    targets.extend(batch['target_text'])\n",
    "\n",
    "        return predictions, targets\n",
    "\n",
    "    def load_checkpoint(self, checkpoint_path: str):\n",
    "        \"\"\"Load a specific checkpoint.\"\"\"\n",
    "        checkpoint_path = Path(checkpoint_path)\n",
    "\n",
    "        if self.is_lora and checkpoint_path.is_dir():\n",
    "            self.model = PeftModel.from_pretrained(self.model, checkpoint_path)\n",
    "            print(f\"LoRA adapter loaded from {checkpoint_path}\")\n",
    "        else:\n",
    "            checkpoint = torch.load(checkpoint_path, map_location=self.device)\n",
    "            self.model.load_state_dict(checkpoint['model_state_dict'] if 'model_state_dict' in checkpoint else checkpoint)\n",
    "            print(f\"Checkpoint loaded from {checkpoint_path}\")\n",
    "        return self\n",
    "\n",
    "print(\"ModelWrapper class loaded successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b776ad8b",
   "metadata": {},
   "source": [
    "### 0.3 Plot Evaluation Class\n",
    "Including all methods we need to plot conference-level paper quality plots."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "67993775",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PlotUtils class loaded successfully\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import os\n",
    "from typing import Dict, List, Sequence, Optional\n",
    "\n",
    "class PlotUtils:\n",
    "    \"\"\"Utility class for creating conference-quality plots.\"\"\"\n",
    "\n",
    "    def __init__(self, style='seaborn-v0_8-paper'):\n",
    "        \"\"\"Initialize plotting style.\"\"\"\n",
    "        try:\n",
    "            plt.style.use(style)\n",
    "        except:\n",
    "            plt.style.use('seaborn-v0_8')\n",
    "\n",
    "        sns.set_palette(\"husl\")\n",
    "        self.colors = sns.color_palette(\"husl\", 10)\n",
    "\n",
    "        # Ensure plots directory exists\n",
    "        os.makedirs('plots', exist_ok=True)\n",
    "\n",
    "    @staticmethod\n",
    "    def _resolve_scales(metric_keys: Sequence[str], scales: Optional[Sequence[float]], default: float = 100.0) -> List[float]:\n",
    "        \"\"\"Resolve scaling factors for metrics.\"\"\"\n",
    "        if scales is None:\n",
    "            return [default] * len(metric_keys)\n",
    "        scale_list = list(scales)\n",
    "        if len(scale_list) != len(metric_keys):\n",
    "            raise ValueError(\"Number of scales must match number of metric keys\")\n",
    "        return [float(value) for value in scale_list]\n",
    "\n",
    "    @staticmethod\n",
    "    def plot_multi_metric_bar(results: Dict[str, Dict], metric_keys: Sequence[str],\n",
    "                              metric_labels: Sequence[str], title: str, save_path: str,\n",
    "                              scales: Optional[Sequence[float]] = None,\n",
    "                              ylabel: Optional[str] = None, ylim: Optional[Sequence[float]] = None):\n",
    "        \"\"\"Plot grouped bar chart for multiple metrics per model.\"\"\"\n",
    "        os.makedirs('plots', exist_ok=True)\n",
    "        models = list(results.keys())\n",
    "        if not models or not metric_keys:\n",
    "            print(\"No data to plot multi-metric bar chart.\")\n",
    "            return\n",
    "\n",
    "        scale_values = PlotUtils._resolve_scales(metric_keys, scales)\n",
    "        if ylabel is None:\n",
    "            if all(abs(scale - 100.0) < 1e-6 for scale in scale_values):\n",
    "                ylabel = 'Score (%)'\n",
    "            else:\n",
    "                ylabel = 'Score'\n",
    "\n",
    "        x = np.arange(len(models))\n",
    "        width = 0.8 / max(1, len(metric_keys))\n",
    "        fig, ax = plt.subplots(figsize=(12, 6))\n",
    "\n",
    "        for idx, (metric, label, scale) in enumerate(zip(metric_keys, metric_labels, scale_values)):\n",
    "            values = [results[model].get(metric, 0.0) * scale for model in models]\n",
    "            offset = (idx - (len(metric_keys) - 1) / 2) * width\n",
    "            bars = ax.bar(x + offset, values, width, label=label)\n",
    "            for bar, value in zip(bars, values):\n",
    "                ax.text(bar.get_x() + bar.get_width() / 2.0, value,\n",
    "                        f'{value:.1f}', ha='center', va='bottom', fontsize=10, fontweight='bold')\n",
    "\n",
    "        ax.set_xticks(x)\n",
    "        ax.set_xticklabels(models)\n",
    "        ax.set_ylabel(ylabel)\n",
    "        ax.set_title(title, fontsize=16, fontweight='bold', pad=20)\n",
    "        ax.legend()\n",
    "        ax.grid(axis='y', alpha=0.3, linestyle='--')\n",
    "        if ylim is not None:\n",
    "            ax.set_ylim(ylim)\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "        plt.close()\n",
    "        print(f\"Plot saved to {save_path}\")\n",
    "\n",
    "    @staticmethod\n",
    "    def plot_metrics_heatmap(results: Dict[str, Dict], title: str, save_path: str,\n",
    "                             metrics: Optional[Sequence[str]] = None,\n",
    "                             metric_labels: Optional[Sequence[str]] = None,\n",
    "                             scales: Optional[Sequence[float]] = None):\n",
    "        \"\"\"Plot metrics heatmap for multiple models.\"\"\"\n",
    "        os.makedirs('plots', exist_ok=True)\n",
    "        models = list(results.keys())\n",
    "        if not models:\n",
    "            print(\"No data to plot heatmap.\")\n",
    "            return\n",
    "\n",
    "        if metrics is None:\n",
    "            metrics = ['accuracy', 'precision', 'recall', 'f1']\n",
    "        if metric_labels is None:\n",
    "            metric_labels = [metric.replace('_', ' ').title() for metric in metrics]\n",
    "        if len(metric_labels) != len(metrics):\n",
    "            raise ValueError(\"Metric labels length must match metrics length\")\n",
    "\n",
    "        scale_values = PlotUtils._resolve_scales(metrics, scales)\n",
    "        data = []\n",
    "        for model in models:\n",
    "            row = []\n",
    "            for metric, scale in zip(metrics, scale_values):\n",
    "                value = results.get(model, {}).get(metric, 0.0)\n",
    "                row.append(value * scale)\n",
    "            data.append(row)\n",
    "        data = np.array(data)\n",
    "\n",
    "        fig, ax = plt.subplots(figsize=(10, 6))\n",
    "        im = ax.imshow(data, cmap='YlGnBu', aspect='auto')\n",
    "\n",
    "        ax.set_xticks(np.arange(len(metrics)))\n",
    "        ax.set_yticks(np.arange(len(models)))\n",
    "        ax.set_xticklabels(metric_labels, fontsize=12)\n",
    "        ax.set_yticklabels(models, fontsize=12)\n",
    "\n",
    "        for i in range(len(models)):\n",
    "            for j in range(len(metrics)):\n",
    "                ax.text(j, i, f'{data[i, j]:.1f}', ha='center', va='center',\n",
    "                        color='black', fontsize=11, fontweight='bold')\n",
    "\n",
    "        cbar = plt.colorbar(im, ax=ax)\n",
    "        if all(abs(scale - 100.0) < 1e-6 for scale in scale_values):\n",
    "            cbar.set_label('Score (%)', rotation=270, labelpad=20)\n",
    "        else:\n",
    "            cbar.set_label('Score', rotation=270, labelpad=20)\n",
    "\n",
    "        ax.set_title(title, fontsize=16, fontweight='bold', pad=20)\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "        plt.close()\n",
    "        print(f\"Plot saved to {save_path}\")\n",
    "\n",
    "    @staticmethod\n",
    "    def plot_training_curves(train_losses: List[float], val_losses: List[float],\n",
    "                             title: str, save_path: str):\n",
    "        \"\"\"Plot training and validation loss curves.\"\"\"\n",
    "        os.makedirs('plots', exist_ok=True)\n",
    "\n",
    "        epochs = list(range(1, len(train_losses) + 1))\n",
    "\n",
    "        fig, ax = plt.subplots(figsize=(10, 6))\n",
    "        ax.plot(epochs, train_losses, marker='o', label='Training Loss', linewidth=2)\n",
    "        ax.plot(epochs, val_losses, marker='s', label='Validation Loss', linewidth=2)\n",
    "\n",
    "        ax.set_xlabel('Epoch', fontsize=14, fontweight='bold')\n",
    "        ax.set_ylabel('Loss', fontsize=14, fontweight='bold')\n",
    "        ax.set_title(title, fontsize=16, fontweight='bold', pad=20)\n",
    "        ax.legend(fontsize=12, loc='best')\n",
    "        ax.grid(alpha=0.3, linestyle='--')\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "        plt.close()\n",
    "\n",
    "        print(f\"Plot saved to {save_path}\")\n",
    "\n",
    "    @staticmethod\n",
    "    def plot_method_metric_bar(method_results: Dict[str, Dict[str, Dict]], metric_key: str,\n",
    "                                title: str, save_path: str, scale: float = 100.0,\n",
    "                                ylabel: Optional[str] = None,\n",
    "                                method_labels: Optional[Dict[str, str]] = None,\n",
    "                                metric_label: Optional[str] = None,\n",
    "                                ylim: Optional[Sequence[float]] = None):\n",
    "        \"\"\"Plot comparison of a single metric across methods and models.\"\"\"\n",
    "        os.makedirs('plots', exist_ok=True)\n",
    "        if not method_results:\n",
    "            print(\"No method data to plot.\")\n",
    "            return\n",
    "\n",
    "        methods = list(method_results.keys())\n",
    "        model_names = sorted({model for res in method_results.values() for model in res.keys()})\n",
    "        if not model_names:\n",
    "            print(\"No model data to plot.\")\n",
    "            return\n",
    "\n",
    "        x = np.arange(len(model_names))\n",
    "        width = 0.8 / max(1, len(methods))\n",
    "        if ylabel is None:\n",
    "            ylabel = 'Score (%)' if abs(scale - 100.0) < 1e-6 else 'Score'\n",
    "        if metric_label is None:\n",
    "            metric_label = metric_key.replace('_', ' ').title()\n",
    "\n",
    "        fig, ax = plt.subplots(figsize=(12, 6))\n",
    "        for idx, method in enumerate(methods):\n",
    "            display_name = method_labels.get(method, method.replace('_', ' ').title()) if method_labels else method.replace('_', ' ').title()\n",
    "            values = []\n",
    "            for model in model_names:\n",
    "                value = method_results[method].get(model, {}).get(metric_key, 0.0)\n",
    "                values.append(value * scale)\n",
    "            offset = (idx - (len(methods) - 1) / 2) * width\n",
    "            bars = ax.bar(x + offset, values, width, label=display_name)\n",
    "            for bar, value in zip(bars, values):\n",
    "                ax.text(bar.get_x() + bar.get_width() / 2.0, value,\n",
    "                        f'{value:.1f}', ha='center', va='bottom', fontsize=10, fontweight='bold')\n",
    "\n",
    "        ax.set_xticks(x)\n",
    "        ax.set_xticklabels(model_names)\n",
    "        ax.set_ylabel(ylabel)\n",
    "        ax.set_title(title, fontsize=16, fontweight='bold', pad=20)\n",
    "        ax.legend()\n",
    "        ax.grid(axis='y', alpha=0.3, linestyle='--')\n",
    "        if ylim is not None:\n",
    "            ax.set_ylim(ylim)\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "        plt.close()\n",
    "        print(f\"Plot saved to {save_path}\")\n",
    "\n",
    "    @staticmethod\n",
    "    def plot_method_comparison(results_dict: Dict[str, Dict[str, Dict]], title: str, save_path: str,\n",
    "                               methods: List[str], metric: str = 'accuracy'):\n",
    "        \"\"\"Compatibility wrapper for grouped method comparisons.\"\"\"\n",
    "        filtered = {method: results_dict[method] for method in methods if method in results_dict}\n",
    "        if not filtered:\n",
    "            print(\"No matching methods to plot.\")\n",
    "            return\n",
    "        PlotUtils.plot_method_metric_bar(\n",
    "            filtered,\n",
    "            metric_key=metric,\n",
    "            title=title,\n",
    "            save_path=save_path,\n",
    "            scale=100.0,\n",
    "            ylabel=f\"{metric.replace('_', ' ').title()} (%)\"\n",
    "        )\n",
    "\n",
    "    @staticmethod\n",
    "    def plot_confusion_matrix(confusion_matrix: np.ndarray, class_names: List[str],\n",
    "                             title: str, save_path: str):\n",
    "        \"\"\"Plot confusion matrix.\"\"\"\n",
    "        os.makedirs('plots', exist_ok=True)\n",
    "\n",
    "        fig, ax = plt.subplots(figsize=(10, 8))\n",
    "        im = ax.imshow(confusion_matrix, cmap='Blues', aspect='auto')\n",
    "\n",
    "        # Set ticks\n",
    "        ax.set_xticks(np.arange(len(class_names)))\n",
    "        ax.set_yticks(np.arange(len(class_names)))\n",
    "        ax.set_xticklabels(class_names, rotation=45, ha='right')\n",
    "        ax.set_yticklabels(class_names)\n",
    "\n",
    "        # Add colorbar\n",
    "        cbar = plt.colorbar(im, ax=ax)\n",
    "        cbar.set_label('Count', rotation=270, labelpad=20)\n",
    "\n",
    "        # Add text annotations\n",
    "        for i in range(len(class_names)):\n",
    "            for j in range(len(class_names)):\n",
    "                text = ax.text(j, i, int(confusion_matrix[i, j]),\n",
    "                             ha=\"center\", va=\"center\", \n",
    "                             color=\"white\" if confusion_matrix[i, j] > confusion_matrix.max()/2 else \"black\")\n",
    "\n",
    "        ax.set_xlabel('Predicted', fontsize=14, fontweight='bold')\n",
    "        ax.set_ylabel('True', fontsize=14, fontweight='bold')\n",
    "        ax.set_title(title, fontsize=16, fontweight='bold', pad=20)\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "        plt.close()\n",
    "\n",
    "        print(f\"Plot saved to {save_path}\")\n",
    "\n",
    "print(\"PlotUtils class loaded successfully\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc4788cc",
   "metadata": {},
   "source": [
    "### 0.4 Hyperparameter Configuration  \n",
    "Define all configurable hyperparameters and provide grid search method.\n",
    "\n",
    "Keep a local JSON called `grid-search-record.json` to save past running results. Each time we run the whole notebook, if we enable grid search, we have to read the JSON file and continue to the next grid search values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "43883a74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "HYPERPARAMETER CONFIGURATION\n",
      "================================================================================\n",
      "batch_size          : 8\n",
      "learning_rate       : 5e-05\n",
      "lora_alpha          : 32\n",
      "lora_dropout        : 0.1\n",
      "lora_r              : 8\n",
      "max_grad_norm       : 1.0\n",
      "max_length          : 512\n",
      "num_epochs          : 3\n",
      "wandb_project       : minecraft-llm\n",
      "warmup_steps        : 100\n",
      "================================================================================\n",
      "HyperparameterConfig class loaded successfully\n"
     ]
    }
   ],
   "source": [
    "import itertools\n",
    "from typing import Dict, List, Any\n",
    "\n",
    "class HyperparameterConfig:\n",
    "    \"\"\"Hyperparameter configuration with grid search support.\"\"\"\n",
    "    \n",
    "    DEFAULT_CONFIG = {\n",
    "        'learning_rate': 5e-5,\n",
    "        'num_epochs': 3,\n",
    "        'batch_size': 8,\n",
    "        'max_length': 512,\n",
    "        'lora_r': 8,\n",
    "        'lora_alpha': 32,\n",
    "        'lora_dropout': 0.1,\n",
    "        'warmup_steps': 100,\n",
    "        'max_grad_norm': 1.0,\n",
    "        'wandb_project': 'minecraft-llm'\n",
    "    }\n",
    "    \n",
    "    def __init__(self, config: Dict[str, Any] = None):\n",
    "        \"\"\"Initialize configuration.\"\"\"\n",
    "        self.config = self.DEFAULT_CONFIG.copy()\n",
    "        if config:\n",
    "            self.config.update(config)\n",
    "        \n",
    "        self.grid_search_file = \"grid-search-record.json\"\n",
    "    \n",
    "    def get_config(self) -> Dict[str, Any]:\n",
    "        \"\"\"Get current configuration.\"\"\"\n",
    "        return self.config.copy()\n",
    "    \n",
    "    def update_config(self, updates: Dict[str, Any]):\n",
    "        \"\"\"Update configuration with new values.\"\"\"\n",
    "        self.config.update(updates)\n",
    "    \n",
    "    def get_grid_search_params(self) -> Dict[str, List[Any]]:\n",
    "        \"\"\"Define grid search parameters.\"\"\"\n",
    "        return {\n",
    "            'learning_rate': [1e-5, 5e-5, 1e-4],\n",
    "            'batch_size': [4, 8, 16],\n",
    "            'lora_r': [4, 8, 16],\n",
    "            'lora_alpha': [16, 32, 64],\n",
    "            'num_epochs': [3, 5]\n",
    "        }\n",
    "    \n",
    "    def generate_grid_configs(self, param_grid: Dict[str, List[Any]] = None) -> List[Dict[str, Any]]:\n",
    "        \"\"\"Generate all combinations of hyperparameters for grid search.\"\"\"\n",
    "        if param_grid is None:\n",
    "            param_grid = self.get_grid_search_params()\n",
    "        \n",
    "        # Get parameter names and values\n",
    "        param_names = list(param_grid.keys())\n",
    "        param_values = [param_grid[name] for name in param_names]\n",
    "        \n",
    "        # Generate all combinations\n",
    "        configs = []\n",
    "        for combination in itertools.product(*param_values):\n",
    "            config = self.config.copy()\n",
    "            for name, value in zip(param_names, combination):\n",
    "                config[name] = value\n",
    "            configs.append(config)\n",
    "        \n",
    "        print(f\"Generated {len(configs)} grid search configurations\")\n",
    "        return configs\n",
    "    \n",
    "    def load_grid_search_record(self) -> Dict[str, Any]:\n",
    "        \"\"\"Load previous grid search results.\"\"\"\n",
    "        try:\n",
    "            record = Utils.load_json(self.grid_search_file)\n",
    "            if record is None:\n",
    "                record = {\n",
    "                    'completed_configs': [],\n",
    "                    'results': [],\n",
    "                    'best_config': None,\n",
    "                    'best_score': -float('inf')\n",
    "                }\n",
    "            return record\n",
    "        except:\n",
    "            return {\n",
    "                'completed_configs': [],\n",
    "                'results': [],\n",
    "                'best_config': None,\n",
    "                'best_score': -float('inf')\n",
    "            }\n",
    "    \n",
    "    def save_grid_search_record(self, record: Dict[str, Any]):\n",
    "        \"\"\"Save grid search results.\"\"\"\n",
    "        Utils.save_json(record, self.grid_search_file)\n",
    "        print(f\"Grid search record saved to {self.grid_search_file}\")\n",
    "    \n",
    "    def get_next_grid_config(self, all_configs: List[Dict[str, Any]]) -> Dict[str, Any]:\n",
    "        \"\"\"Get the next configuration to try in grid search.\"\"\"\n",
    "        record = self.load_grid_search_record()\n",
    "        completed = record['completed_configs']\n",
    "        \n",
    "        # Find first config not yet completed\n",
    "        for config in all_configs:\n",
    "            config_key = self._config_to_key(config)\n",
    "            if config_key not in completed:\n",
    "                return config\n",
    "        \n",
    "        return None  # All configs completed\n",
    "    \n",
    "    def update_grid_search_result(self, config: Dict[str, Any], result: Dict[str, Any], score: float):\n",
    "        \"\"\"Update grid search record with a new result.\"\"\"\n",
    "        record = self.load_grid_search_record()\n",
    "        \n",
    "        config_key = self._config_to_key(config)\n",
    "        record['completed_configs'].append(config_key)\n",
    "        record['results'].append({\n",
    "            'config': config,\n",
    "            'result': result,\n",
    "            'score': score\n",
    "        })\n",
    "        \n",
    "        # Update best config if this is better\n",
    "        if score > record['best_score']:\n",
    "            record['best_config'] = config\n",
    "            record['best_score'] = score\n",
    "            print(f\"New best configuration found! Score: {score:.4f}\")\n",
    "        \n",
    "        self.save_grid_search_record(record)\n",
    "    \n",
    "    def _config_to_key(self, config: Dict[str, Any]) -> str:\n",
    "        \"\"\"Convert config to a unique string key.\"\"\"\n",
    "        # Only use grid search parameters\n",
    "        grid_params = self.get_grid_search_params()\n",
    "        key_parts = []\n",
    "        for param in sorted(grid_params.keys()):\n",
    "            if param in config:\n",
    "                key_parts.append(f\"{param}={config[param]}\")\n",
    "        return \"_\".join(key_parts)\n",
    "    \n",
    "    def print_config(self):\n",
    "        \"\"\"Print current configuration.\"\"\"\n",
    "        print(\"=\" * 80)\n",
    "        print(\"HYPERPARAMETER CONFIGURATION\")\n",
    "        print(\"=\" * 80)\n",
    "        for key, value in sorted(self.config.items()):\n",
    "            print(f\"{key:20s}: {value}\")\n",
    "        print(\"=\" * 80)\n",
    "    \n",
    "    def print_grid_search_summary(self):\n",
    "        \"\"\"Print summary of grid search results.\"\"\"\n",
    "        record = self.load_grid_search_record()\n",
    "        \n",
    "        print(\"=\" * 80)\n",
    "        print(\"GRID SEARCH SUMMARY\")\n",
    "        print(\"=\" * 80)\n",
    "        print(f\"Completed configurations: {len(record['completed_configs'])}\")\n",
    "        print(f\"Best score: {record['best_score']:.4f}\")\n",
    "        \n",
    "        if record['best_config']:\n",
    "            print(\"\\nBest configuration:\")\n",
    "            for key, value in sorted(record['best_config'].items()):\n",
    "                if key in self.get_grid_search_params():\n",
    "                    print(f\"  {key:20s}: {value}\")\n",
    "        print(\"=\" * 80)\n",
    "\n",
    "# Initialize default configuration\n",
    "config = HyperparameterConfig()\n",
    "config.print_config()\n",
    "\n",
    "print(\"HyperparameterConfig class loaded successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c73327ca",
   "metadata": {},
   "source": [
    "1.1 load model via transformers, we pick Qwen3-0.6B and Qwen3-4B"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2d667e0",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9a43700f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available models: ['qwen3-0.6b', 'qwen3-4b']\n",
      "Using device: cuda\n",
      "Call get_model_wrapper('<model_key>') to load a model when needed.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import os\n",
    "from typing import Dict, Optional\n",
    "\n",
    "import torch\n",
    "\n",
    "MODEL_PATHS: Dict[str, str] = {\n",
    "    \"qwen3-0.6b\": \"models/Qwen3-0.6B\",\n",
    "    \"qwen3-4b\": \"models/Qwen3-4B\",\n",
    "}\n",
    "\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "WANDB_ENABLED = False  # Toggle to True to log to Weights & Biases when credentials are available\n",
    "\n",
    "os.environ.setdefault(\"WANDB_MODE\", \"offline\")\n",
    "os.environ.setdefault(\"WANDB_SILENT\", \"true\")\n",
    "\n",
    "loaded_models: Dict[str, ModelWrapper] = {}\n",
    "\n",
    "def get_model_wrapper(model_key: str, *, use_lora: bool = False, lora_config: Optional[dict] = None, force_reload: bool = False) -> ModelWrapper:\n",
    "    \"\"\"Return a ModelWrapper for the requested model, loading weights on demand.\"\"\"\n",
    "    if model_key not in MODEL_PATHS:\n",
    "        raise KeyError(f\"Unknown model key '{model_key}'. Available: {list(MODEL_PATHS.keys())}\")\n",
    "\n",
    "    wrapper = loaded_models.get(model_key)\n",
    "    needs_reload = force_reload or wrapper is None or getattr(wrapper, \"model\", None) is None or (use_lora and not getattr(wrapper, \"is_lora\", False))\n",
    "\n",
    "    if wrapper is None:\n",
    "        wrapper = ModelWrapper(model_name=MODEL_PATHS[model_key], device=DEVICE)\n",
    "        needs_reload = True\n",
    "\n",
    "    if needs_reload:\n",
    "        wrapper.load_model(use_lora=use_lora, lora_config=lora_config)\n",
    "        loaded_models[model_key] = wrapper\n",
    "\n",
    "    return wrapper\n",
    "\n",
    "def release_model(model_key: str) -> None:\n",
    "    \"\"\"Remove a loaded model from memory to free GPU/CPU resources.\"\"\"\n",
    "    wrapper = loaded_models.pop(model_key, None)\n",
    "    if wrapper is None:\n",
    "        return\n",
    "\n",
    "    model = getattr(wrapper, \"model\", None)\n",
    "    try:\n",
    "        if model is not None:\n",
    "            try:\n",
    "                model.to(\"cpu\")\n",
    "            except Exception as move_exc:\n",
    "                print(f\"Warning: unable to move model for {model_key} to CPU: {move_exc}\")\n",
    "        # Explicitly delete model reference to encourage cleanup\n",
    "        del model\n",
    "    except Exception as exc:\n",
    "        print(f\"Warning: cleanup for model {model_key} raised an exception: {exc}\")\n",
    "\n",
    "    wrapper.model = None\n",
    "    wrapper.tokenizer = None\n",
    "    wrapper.is_lora = False\n",
    "\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "        if hasattr(torch.cuda, \"ipc_collect\"):\n",
    "            torch.cuda.ipc_collect()\n",
    "    try:\n",
    "        import gc\n",
    "        gc.collect()\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    wrapper = None\n",
    "\n",
    "def release_all_models() -> None:\n",
    "    \"\"\"Release all cached models to ensure GPU memory is freed.\"\"\"\n",
    "    for cached_key in list(loaded_models.keys()):\n",
    "        release_model(cached_key)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "        if hasattr(torch.cuda, \"ipc_collect\"):\n",
    "            torch.cuda.ipc_collect()\n",
    "    try:\n",
    "        import gc\n",
    "        gc.collect()\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "print(f\"Available models: {list(MODEL_PATHS.keys())}\")\n",
    "print(f\"Using device: {DEVICE}\")\n",
    "print(\"Call get_model_wrapper('<model_key>') to load a model when needed.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "142b504e",
   "metadata": {},
   "source": [
    "1.2 load custom data from local datasets dir, the data with 3 types of data, \n",
    "\n",
    "- x : current frame in ascii art, \n",
    "- y: current action token, \n",
    "- z: next frame ascii art, \n",
    "\n",
    "all in plain text format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "706c7bbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading 9 frames from datasets/minecraft/data\n",
      "Created 8 training pairs\n",
      "Loaded 8 sequential frame/action pairs from datasets/minecraft/data.\n",
      "Unique actions: ['straight: forward\\npan: left\\njump: jump\\n']\n",
      "================================================================================\n",
      "Example 0\n",
      "- Current Frame (x):\n",
      "|air|grass block|dirt|\n",
      "|air|grass block|dirt|\n",
      "|air|grass block|dirt|\n",
      "|air|air|dirt|\n",
      "|air|air|dirt|\n",
      "|air|air|dirt|\n",
      "|air|air|grass block|\n",
      "|air|air|grass block|\n",
      "|air|air|grass block|\n",
      "\n",
      "- Action (y):\n",
      "straight: forward\n",
      "pan: left\n",
      "jump: jump\n",
      "\n",
      "- Next Frame (z):\n",
      "|air|grass block|dirt|\n",
      "|air|grass block|dirt|\n",
      "|air|grass block|dirt|\n",
      "|air|air|dirt|\n",
      "|air|air|dirt|\n",
      "|air|air|dirt|\n",
      "|air|air|grass block|\n",
      "|air|air|grass block|\n",
      "|air|air|grass block|\n",
      "\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "from minecraft_dataset import MinecraftDataset\n",
    "\n",
    "DATA_DIR = Path(\"datasets/minecraft/data\")\n",
    "\n",
    "full_dataset = MinecraftDataset(\n",
    "    data_dir=str(DATA_DIR),\n",
    "    max_length=config.get_config()[\"max_length\"]\n",
    ")\n",
    "\n",
    "TOTAL_PAIRS = len(full_dataset)\n",
    "UNIQUE_ACTIONS = sorted({pair[\"y\"] for pair in full_dataset.data_pairs})\n",
    "\n",
    "print(f\"Loaded {TOTAL_PAIRS} sequential frame/action pairs from {DATA_DIR}.\")\n",
    "print(f\"Unique actions: {UNIQUE_ACTIONS}\")\n",
    "\n",
    "\n",
    "def preview_dataset_example(idx: int = 0) -> None:\n",
    "    \"\"\"Print a dataset example for quick inspection.\"\"\"\n",
    "    example = full_dataset[idx]\n",
    "    print(\"=\" * 80)\n",
    "    print(f\"Example {idx}\")\n",
    "    print(\"- Current Frame (x):\")\n",
    "    print(example[\"x\"])\n",
    "    print(\"- Action (y):\")\n",
    "    print(example[\"y\"])\n",
    "    print(\"- Next Frame (z):\")\n",
    "    print(example[\"z\"])\n",
    "    print(\"=\" * 80)\n",
    "\n",
    "\n",
    "preview_dataset_example(0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac3e0933",
   "metadata": {},
   "source": [
    "1.3 split loaded data to all, train, val and test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a5a6280e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset split sizes: {'train': 5, 'val': 1, 'test': 2}\n",
      "Using 5 sequential training frames as context (up to 100 frames).\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import re\n",
    "from difflib import SequenceMatcher\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.metrics import precision_recall_fscore_support, confusion_matrix\n",
    "\n",
    "TOTAL_SAMPLES = len(full_dataset)\n",
    "indices = list(range(TOTAL_SAMPLES))\n",
    "\n",
    "if TOTAL_SAMPLES < 3:\n",
    "    train_indices = indices[:max(1, TOTAL_SAMPLES - 2)]\n",
    "    val_indices = indices[len(train_indices):len(train_indices) + (1 if TOTAL_SAMPLES - len(train_indices) > 1 else 0)]\n",
    "    test_indices = indices[len(train_indices) + len(val_indices):]\n",
    "else:\n",
    "    train_size = max(1, int(np.floor(0.7 * TOTAL_SAMPLES)))\n",
    "    val_size = max(1, int(np.floor(0.15 * TOTAL_SAMPLES)))\n",
    "    remaining = TOTAL_SAMPLES - train_size - val_size\n",
    "\n",
    "    if remaining < 1:\n",
    "        deficit = 1 - remaining\n",
    "        if val_size - deficit >= 1:\n",
    "            val_size -= deficit\n",
    "        else:\n",
    "            deficit -= (val_size - 1)\n",
    "            val_size = 1\n",
    "            train_size = max(1, train_size - deficit)\n",
    "        remaining = 1\n",
    "\n",
    "    test_size = remaining\n",
    "    train_indices = indices[:train_size]\n",
    "    val_start = train_size\n",
    "    val_indices = indices[val_start:val_start + val_size]\n",
    "    test_indices = indices[val_start + val_size:val_start + val_size + test_size]\n",
    "\n",
    "SPLITS = {\n",
    "    \"train\": train_indices,\n",
    "    \"val\": val_indices,\n",
    "    \"test\": test_indices,\n",
    "}\n",
    "\n",
    "print(\"Dataset split sizes:\", {split: len(idxs) for split, idxs in SPLITS.items()})\n",
    "\n",
    "PROMPT_CONTEXT_LENGTH = 100\n",
    "CONTEXT_EXAMPLES = [\n",
    "    full_dataset.data_pairs[i]\n",
    "    for i in SPLITS[\"train\"][-PROMPT_CONTEXT_LENGTH:]\n",
    "]\n",
    "CONTEXT_EXAMPLE_COUNT = len(CONTEXT_EXAMPLES)\n",
    "MAX_NEW_TOKENS = 256\n",
    "\n",
    "print(\n",
    "    f\"Using {CONTEXT_EXAMPLE_COUNT} sequential training frames as context (up to {PROMPT_CONTEXT_LENGTH} frames).\"\n",
    ")\n",
    "\n",
    "\n",
    "def create_dataset_subset(indices, *, context_examples=None):\n",
    "    \"\"\"Create an in-memory subset of the Minecraft dataset for custom splits.\"\"\"\n",
    "    subset = MinecraftDataset.__new__(MinecraftDataset)\n",
    "    subset.data_dir = full_dataset.data_dir\n",
    "    subset.tokenizer = None\n",
    "    subset.max_length = full_dataset.max_length\n",
    "    subset.context_examples = list(context_examples or [])\n",
    "    subset.data_pairs = [full_dataset.data_pairs[i] for i in indices]\n",
    "    return subset\n",
    "\n",
    "\n",
    "def build_dataloader(indices, tokenizer, task_type, *, batch_size=1, shuffle=False, context_examples=None):\n",
    "    \"\"\"Create a dataloader for the requested task and split.\"\"\"\n",
    "    dataset_subset = create_dataset_subset(indices, context_examples=context_examples)\n",
    "    collate_fn = (\n",
    "        dataset_subset.collate_frame_reconstruction\n",
    "        if task_type == \"frame_reconstruction\"\n",
    "        else dataset_subset.collate_action_recognition\n",
    "    )\n",
    "    return DataLoader(\n",
    "        dataset_subset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=shuffle,\n",
    "        collate_fn=lambda batch: collate_fn(batch, tokenizer),\n",
    "    )\n",
    "\n",
    "\n",
    "class Word2VecManager:\n",
    "    \"\"\"Lightweight Word2Vec-style embeddings trained on local action text.\"\"\"\n",
    "\n",
    "    def __init__(self, texts, embedding_dim=32, window_size=2, epochs=300, lr=5e-3):\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.window_size = window_size\n",
    "        self.epochs = epochs\n",
    "        self.lr = lr\n",
    "        self._device = torch.device(\"cpu\")\n",
    "\n",
    "        self.sentences = [self.tokenize(text) for text in texts if text and text.strip()]\n",
    "        self.vocab = []\n",
    "        self.word_to_idx = {}\n",
    "        self.idx_to_word = {}\n",
    "        self.input_embeddings = None\n",
    "        self.output_layer = None\n",
    "        self.training_pairs = []\n",
    "\n",
    "        if self.sentences:\n",
    "            self._build_vocab()\n",
    "            self._build_training_pairs()\n",
    "            if self.training_pairs:\n",
    "                self._train_embeddings()\n",
    "\n",
    "    @staticmethod\n",
    "    def tokenize(text):\n",
    "        return [tok for tok in re.findall(r\"[A-Za-z0-9_]+\", text.lower())]\n",
    "\n",
    "    def _build_vocab(self):\n",
    "        vocab = sorted({token for sent in self.sentences for token in sent})\n",
    "        self.vocab = vocab\n",
    "        self.word_to_idx = {word: idx for idx, word in enumerate(vocab)}\n",
    "        self.idx_to_word = {idx: word for word, idx in self.word_to_idx.items()}\n",
    "\n",
    "    def _build_training_pairs(self):\n",
    "        pairs = []\n",
    "        for sent in self.sentences:\n",
    "            if not sent:\n",
    "                continue\n",
    "            for center_idx, token in enumerate(sent):\n",
    "                center_id = self.word_to_idx[token]\n",
    "                start = max(0, center_idx - self.window_size)\n",
    "                end = min(len(sent), center_idx + self.window_size + 1)\n",
    "                for context_pos in range(start, end):\n",
    "                    if context_pos == center_idx:\n",
    "                        continue\n",
    "                    context_token = sent[context_pos]\n",
    "                    pairs.append((center_id, self.word_to_idx[context_token]))\n",
    "        self.training_pairs = pairs\n",
    "\n",
    "    def _train_embeddings(self):\n",
    "        torch.manual_seed(42)\n",
    "        vocab_size = len(self.vocab)\n",
    "        self.input_embeddings = torch.nn.Embedding(vocab_size, self.embedding_dim).to(self._device)\n",
    "        self.output_layer = torch.nn.Linear(self.embedding_dim, vocab_size, bias=False).to(self._device)\n",
    "\n",
    "        optimizer = torch.optim.Adam(\n",
    "            list(self.input_embeddings.parameters()) + list(self.output_layer.parameters()),\n",
    "            lr=self.lr,\n",
    "        )\n",
    "        loss_fn = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "        for epoch in range(self.epochs):\n",
    "            total_loss = 0.0\n",
    "            for center_idx, context_idx in self.training_pairs:\n",
    "                center_tensor = torch.tensor([center_idx], dtype=torch.long, device=self._device)\n",
    "                context_tensor = torch.tensor([context_idx], dtype=torch.long, device=self._device)\n",
    "\n",
    "                logits = self.output_layer(self.input_embeddings(center_tensor))\n",
    "                loss = loss_fn(logits, context_tensor)\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                total_loss += loss.item()\n",
    "\n",
    "            if total_loss < 1e-6:\n",
    "                break\n",
    "\n",
    "    def encode(self, text):\n",
    "        if not self.training_pairs or self.input_embeddings is None:\n",
    "            return np.zeros(self.embedding_dim, dtype=float)\n",
    "\n",
    "        tokens = self.tokenize(text)\n",
    "        ids = [self.word_to_idx[token] for token in tokens if token in self.word_to_idx]\n",
    "        if not ids:\n",
    "            return np.zeros(self.embedding_dim, dtype=float)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            tensor = torch.tensor(ids, dtype=torch.long, device=self._device)\n",
    "            embeddings = self.input_embeddings(tensor)\n",
    "            vector = embeddings.mean(dim=0)\n",
    "        return vector.cpu().numpy()\n",
    "\n",
    "    def cosine_similarity(self, text_a, text_b):\n",
    "        vec_a = self.encode(text_a)\n",
    "        vec_b = self.encode(text_b)\n",
    "        denom = np.linalg.norm(vec_a) * np.linalg.norm(vec_b)\n",
    "        if denom == 0.0:\n",
    "            return 0.0\n",
    "        return float(np.dot(vec_a, vec_b) / denom)\n",
    "\n",
    "\n",
    "def regex_fullmatch(text: str, pattern: str) -> bool:\n",
    "    \"\"\"Return True when text fully matches pattern using regex with fallback to literal match.\"\"\"\n",
    "    text = text.strip()\n",
    "    pattern = pattern.strip()\n",
    "    if pattern == \"\":\n",
    "        return text == pattern\n",
    "\n",
    "    flags = re.IGNORECASE | re.DOTALL\n",
    "\n",
    "    try:\n",
    "        compiled = re.compile(pattern, flags)\n",
    "        if compiled.fullmatch(text):\n",
    "            return True\n",
    "    except re.error:\n",
    "        compiled = None\n",
    "\n",
    "    literal_compiled = re.compile(re.escape(pattern), flags)\n",
    "    return bool(literal_compiled.fullmatch(text))\n",
    "\n",
    "\n",
    "ACTION_EMBEDDER = Word2VecManager([pair[\"y\"] for pair in full_dataset.data_pairs])\n",
    "\n",
    "\n",
    "def compute_text_metrics(predictions, targets, *, task_type: str):\n",
    "    \"\"\"Compute metrics for text generation tasks, including regex and similarity metrics.\"\"\"\n",
    "    if not targets:\n",
    "        return {\n",
    "            \"accuracy\": 0.0,\n",
    "            \"precision\": 0.0,\n",
    "            \"recall\": 0.0,\n",
    "            \"f1\": 0.0,\n",
    "            \"labels\": [],\n",
    "            \"confusion_matrix\": [],\n",
    "            \"regex_matches\": 0,\n",
    "            \"strict_match_accuracy\": 0.0,\n",
    "            \"reconstruction_accuracy\": 0.0,\n",
    "            \"word2vec_cosine\": 0.0,\n",
    "        }\n",
    "\n",
    "    regex_matches = []\n",
    "    normalized_predictions = []\n",
    "    reconstruction_scores = []\n",
    "    cosine_scores = []\n",
    "\n",
    "    for pred, target in zip(predictions, targets):\n",
    "        match = regex_fullmatch(pred, target)\n",
    "        regex_matches.append(1 if match else 0)\n",
    "        normalized_predictions.append(target if match else pred)\n",
    "\n",
    "        if task_type == \"frame_reconstruction\":\n",
    "            score = SequenceMatcher(None, target, pred).ratio()\n",
    "            reconstruction_scores.append(score)\n",
    "        elif task_type == \"action_recognition\":\n",
    "            cosine_scores.append(ACTION_EMBEDDER.cosine_similarity(pred, target))\n",
    "\n",
    "    strict_accuracy = float(np.mean(regex_matches))\n",
    "\n",
    "    metrics = {\n",
    "        \"regex_matches\": int(sum(regex_matches)),\n",
    "        \"strict_match_accuracy\": strict_accuracy,\n",
    "    }\n",
    "\n",
    "    if task_type == \"frame_reconstruction\":\n",
    "        reconstruction_accuracy = float(np.mean(reconstruction_scores)) if reconstruction_scores else 0.0\n",
    "        metrics.update({\n",
    "            \"accuracy\": reconstruction_accuracy,\n",
    "            \"precision\": 0.0,\n",
    "            \"recall\": 0.0,\n",
    "            \"f1\": 0.0,\n",
    "            \"labels\": [],\n",
    "            \"confusion_matrix\": [],\n",
    "            \"reconstruction_accuracy\": reconstruction_accuracy,\n",
    "            \"reconstruction_scores\": reconstruction_scores,\n",
    "            \"word2vec_cosine\": 0.0,\n",
    "        })\n",
    "    else:\n",
    "        label_space = sorted(set(targets + normalized_predictions))\n",
    "        label_to_idx = {label: idx for idx, label in enumerate(label_space)}\n",
    "        y_true = [label_to_idx[t] for t in targets]\n",
    "        y_pred = [label_to_idx[p] for p in normalized_predictions]\n",
    "\n",
    "        precision, recall, f1, _ = precision_recall_fscore_support(\n",
    "            y_true,\n",
    "            y_pred,\n",
    "            average=\"macro\",\n",
    "            zero_division=0,\n",
    "        )\n",
    "        conf = confusion_matrix(\n",
    "            y_true,\n",
    "            y_pred,\n",
    "            labels=list(range(len(label_space))),\n",
    "        )\n",
    "\n",
    "        cosine_mean = float(np.mean(cosine_scores)) if cosine_scores else 0.0\n",
    "\n",
    "        metrics.update({\n",
    "            \"accuracy\": strict_accuracy,\n",
    "            \"precision\": float(precision),\n",
    "            \"recall\": float(recall),\n",
    "            \"f1\": float(f1),\n",
    "            \"labels\": label_space,\n",
    "            \"confusion_matrix\": conf.astype(int).tolist(),\n",
    "            \"reconstruction_accuracy\": 0.0,\n",
    "            \"word2vec_cosine\": cosine_mean,\n",
    "            \"word2vec_scores\": cosine_scores,\n",
    "        })\n",
    "\n",
    "    return metrics\n",
    "\n",
    "\n",
    "def evaluate_wrapper(wrapper, model_key, task_type, indices, *, context_examples=None, batch_size=1, max_new_tokens=MAX_NEW_TOKENS):\n",
    "    \"\"\"Run evaluation for a pre-loaded wrapper.\"\"\"\n",
    "    dataloader = build_dataloader(\n",
    "        indices,\n",
    "        wrapper.tokenizer,\n",
    "        task_type,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=False,\n",
    "        context_examples=context_examples,\n",
    "    )\n",
    "    predictions, targets = wrapper.evaluate(dataloader, max_new_tokens=max_new_tokens)\n",
    "    metrics = compute_text_metrics(predictions, targets, task_type=task_type)\n",
    "    metrics.update({\n",
    "        \"model\": model_key,\n",
    "        \"num_samples\": len(targets),\n",
    "        \"num_context_examples\": len(context_examples) if context_examples else 0,\n",
    "        \"predictions\": predictions,\n",
    "        \"targets\": targets,\n",
    "    })\n",
    "    return metrics\n",
    "\n",
    "\n",
    "def evaluate_model_on_task(model_key, task_type, indices, *, context_examples=None, batch_size=1, max_new_tokens=MAX_NEW_TOKENS, use_lora=False, lora_config=None):\n",
    "    \"\"\"Convenience wrapper that loads a model, runs evaluation, and returns metrics.\"\"\"\n",
    "    wrapper = get_model_wrapper(model_key, use_lora=use_lora, lora_config=lora_config)\n",
    "    metrics = evaluate_wrapper(\n",
    "        wrapper,\n",
    "        model_key,\n",
    "        task_type,\n",
    "        indices,\n",
    "        context_examples=context_examples,\n",
    "        batch_size=batch_size,\n",
    "        max_new_tokens=max_new_tokens,\n",
    "    )\n",
    "    return metrics\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f4f2bbb",
   "metadata": {},
   "source": [
    "2.1 evaluate both model on all data, for next frame reconstraction task, input x and y, output z, save result to 2.1-result.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4bcb67ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating frame reconstruction with 5 sequential prompt examples.\n",
      "Loading model: models/Qwen3-0.6B\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded on cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:   0%|          | 0/2 [00:00<?, ?it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Evaluating: 100%|██████████| 2/2 [00:05<00:00,  2.65s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model: models/Qwen3-4B\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "16310b4ac6a242f7810580bdff789634",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded on cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 2/2 [00:10<00:00,  5.45s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved zero-shot frame reconstruction results to 2.1-result.json\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'qwen3-0.6b': {'regex_matches': 0,\n",
       "  'strict_match_accuracy': 0.0,\n",
       "  'accuracy': 0.055205047318611984,\n",
       "  'precision': 0.0,\n",
       "  'recall': 0.0,\n",
       "  'f1': 0.0,\n",
       "  'labels': [],\n",
       "  'confusion_matrix': [],\n",
       "  'reconstruction_accuracy': 0.055205047318611984,\n",
       "  'reconstruction_scores': [0.055205047318611984, 0.055205047318611984],\n",
       "  'word2vec_cosine': 0.0,\n",
       "  'model': 'qwen3-0.6b',\n",
       "  'num_samples': 2,\n",
       "  'num_context_examples': 5},\n",
       " 'qwen3-4b': {'regex_matches': 0,\n",
       "  'strict_match_accuracy': 0.0,\n",
       "  'accuracy': 0.07267221801665405,\n",
       "  'precision': 0.0,\n",
       "  'recall': 0.0,\n",
       "  'f1': 0.0,\n",
       "  'labels': [],\n",
       "  'confusion_matrix': [],\n",
       "  'reconstruction_accuracy': 0.07267221801665405,\n",
       "  'reconstruction_scores': [0.07267221801665405, 0.07267221801665405],\n",
       "  'word2vec_cosine': 0.0,\n",
       "  'model': 'qwen3-4b',\n",
       "  'num_samples': 2,\n",
       "  'num_context_examples': 5}}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "from pathlib import Path\n",
    "\n",
    "frame_prompt_examples = CONTEXT_EXAMPLES\n",
    "print(\n",
    "    f\"Evaluating frame reconstruction with {len(frame_prompt_examples)} sequential prompt examples.\"\n",
    ")\n",
    "\n",
    "frame_results = {}\n",
    "for model_key in MODEL_PATHS:\n",
    "    metrics = evaluate_model_on_task(\n",
    "        model_key,\n",
    "        \"frame_reconstruction\",\n",
    "        SPLITS[\"test\"],\n",
    "        context_examples=frame_prompt_examples,\n",
    "        batch_size=1,\n",
    "    )\n",
    "    frame_results[model_key] = {\n",
    "        k: v for k, v in metrics.items() if k not in {\"predictions\", \"targets\"}\n",
    "    }\n",
    "    release_model(model_key)\n",
    "\n",
    "frame_results_path = Path(\"2.1-result.json\")\n",
    "Utils.save_json(frame_results, frame_results_path)\n",
    "print(f\"Saved zero-shot frame reconstruction results to {frame_results_path}\")\n",
    "\n",
    "release_all_models()\n",
    "\n",
    "frame_results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b314d905",
   "metadata": {},
   "source": [
    "2.2 plot the result of the evalutation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0dcffcde",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Plot saved to plots/2.2-frame-metric-bars.png\n",
      "Plot saved to plots/2.2-frame-heatmap.png\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'qwen3-0.6b': {'regex_matches': 0,\n",
       "  'strict_match_accuracy': 0.0,\n",
       "  'accuracy': 0.055205047318611984,\n",
       "  'precision': 0.0,\n",
       "  'recall': 0.0,\n",
       "  'f1': 0.0,\n",
       "  'labels': [],\n",
       "  'confusion_matrix': [],\n",
       "  'reconstruction_accuracy': 0.055205047318611984,\n",
       "  'reconstruction_scores': [0.055205047318611984, 0.055205047318611984],\n",
       "  'word2vec_cosine': 0.0,\n",
       "  'model': 'qwen3-0.6b',\n",
       "  'num_samples': 2,\n",
       "  'num_context_examples': 5},\n",
       " 'qwen3-4b': {'regex_matches': 0,\n",
       "  'strict_match_accuracy': 0.0,\n",
       "  'accuracy': 0.07267221801665405,\n",
       "  'precision': 0.0,\n",
       "  'recall': 0.0,\n",
       "  'f1': 0.0,\n",
       "  'labels': [],\n",
       "  'confusion_matrix': [],\n",
       "  'reconstruction_accuracy': 0.07267221801665405,\n",
       "  'reconstruction_scores': [0.07267221801665405, 0.07267221801665405],\n",
       "  'word2vec_cosine': 0.0,\n",
       "  'model': 'qwen3-4b',\n",
       "  'num_samples': 2,\n",
       "  'num_context_examples': 5}}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "plot_utils = PlotUtils()\n",
    "frame_results = Utils.load_json(\"2.1-result.json\") or {}\n",
    "\n",
    "if not frame_results:\n",
    "    print(\"No zero-shot frame reconstruction results found. Run cell 2.1 first.\")\n",
    "else:\n",
    "    PlotUtils.plot_multi_metric_bar(\n",
    "        frame_results,\n",
    "        metric_keys=[\"strict_match_accuracy\", \"reconstruction_accuracy\"],\n",
    "        metric_labels=[\"Strict Match Accuracy\", \"Reconstruction Accuracy\"],\n",
    "        title=\"Zero-Shot Frame Reconstruction Metrics\",\n",
    "        save_path=\"plots/2.2-frame-metric-bars.png\",\n",
    "        scales=[100.0, 100.0],\n",
    "        ylabel=\"Score (%)\",\n",
    "        ylim=(0, 100),\n",
    "    )\n",
    "    PlotUtils.plot_metrics_heatmap(\n",
    "        frame_results,\n",
    "        \"Zero-Shot Frame Reconstruction Heatmap\",\n",
    "        \"plots/2.2-frame-heatmap.png\",\n",
    "        metrics=[\"strict_match_accuracy\", \"reconstruction_accuracy\"],\n",
    "        metric_labels=[\"Strict Match Accuracy (%)\", \"Reconstruction Accuracy (%)\"],\n",
    "        scales=[100.0, 100.0],\n",
    "    )\n",
    "\n",
    "frame_results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fbfbfb4",
   "metadata": {},
   "source": [
    "2.3 evaluate both model on all data for the action recognition task, input x and z, output y, save result to 2.3-result.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "09895268",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model: models/Qwen3-0.6B\n",
      "Model loaded on cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 2/2 [00:05<00:00,  2.60s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model: models/Qwen3-4B\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "87fc87d91854499583611102ce6d9a78",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded on cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 2/2 [00:10<00:00,  5.46s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved zero-shot action recognition results to 2.3-result.json\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'qwen3-0.6b': {'regex_matches': 0,\n",
       "  'strict_match_accuracy': 0.0,\n",
       "  'accuracy': 0.0,\n",
       "  'precision': 0.0,\n",
       "  'recall': 0.0,\n",
       "  'f1': 0.0,\n",
       "  'labels': ['irt|\\n|air|air|grass block|\\n|air|air|grass block|\\n|air|air|grass block|\\n\\n\\nFrame:\\n|air|grass block|dirt|\\n|air|grass block|dirt|\\n|air|grass block|dirt|\\n|air|air|dirt|\\n|air|air|dirt|\\n|air|air|dirt|\\n|air|air|grass block|\\n|air|air|grass block|\\n|air|air|grass block|\\n\\n\\nAction:\\nstraight: forward\\npan: left\\njump: jump\\n\\n\\nFrame:\\n|air|grass block|dirt|\\n|air|grass block|dirt|\\n|air|grass block|dirt|\\n|air|air|dirt|\\n|air|air|dirt|\\n|air|air|dirt|\\n|air|air|grass block|\\n|air|air|grass block|\\n|air|air|grass block|\\n\\n\\nFrame:\\n|air|grass block|dirt|\\n|air|grass block|dirt|\\n|air|grass block|dirt|\\n|air|air|dirt|\\n|air|air|dirt|\\n|air|air|dirt|\\n|air|air|grass',\n",
       "   'straight: forward\\npan: left\\njump: jump'],\n",
       "  'confusion_matrix': [[0, 0], [2, 0]],\n",
       "  'reconstruction_accuracy': 0.0,\n",
       "  'word2vec_cosine': 0.9999999403953552,\n",
       "  'word2vec_scores': [0.9999999403953552, 0.9999999403953552],\n",
       "  'model': 'qwen3-0.6b',\n",
       "  'num_samples': 2,\n",
       "  'num_context_examples': 5},\n",
       " 'qwen3-4b': {'regex_matches': 0,\n",
       "  'strict_match_accuracy': 0.0,\n",
       "  'accuracy': 0.0,\n",
       "  'precision': 0.0,\n",
       "  'recall': 0.0,\n",
       "  'f1': 0.0,\n",
       "  'labels': ['irt|\\n|air|air|grass block|\\n|air|air|grass block|\\n|air|air|grass block|\\n\\n\\nFrame:\\n|air|grass block|dirt|\\n|air|grass block|dirt|\\n|air|grass block|dirt|\\n|air|air|dirt|\\n|air|air|dirt|\\n|air|air|dirt|\\n|air|air|grass block|\\n|air|air|grass block|\\n|air|air|grass block|\\n\\n\\nAction:\\nstraight: forward\\npan: left\\njump: jump\\nOkay, I need to figure out the correct action based on the given frames. Let me look at the pattern. The frames repeat the same structure, and the actions are always \"straight: forward\", \"pan: left\", \"jump: jump\". Since the pattern alternates between Frame and Action, and the last frame provided is the same as the previous ones, the next action should be the third one in the sequence. The actions cycle through forward, left, jump. So after two actions (forward and left), the next should be jump. Therefore, the action is \"jump: jump\".\\nstraight: forward\\npan: left\\njump: jump\\njump: jump\\n\\nWait',\n",
       "   'straight: forward\\npan: left\\njump: jump'],\n",
       "  'confusion_matrix': [[0, 0], [2, 0]],\n",
       "  'reconstruction_accuracy': 0.0,\n",
       "  'word2vec_cosine': 0.9676053524017334,\n",
       "  'word2vec_scores': [0.9676053524017334, 0.9676053524017334],\n",
       "  'model': 'qwen3-4b',\n",
       "  'num_samples': 2,\n",
       "  'num_context_examples': 5}}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "from pathlib import Path\n",
    "\n",
    "action_results = {}\n",
    "for model_key in MODEL_PATHS:\n",
    "    metrics = evaluate_model_on_task(\n",
    "        model_key,\n",
    "        \"action_recognition\",\n",
    "        SPLITS[\"test\"],\n",
    "        context_examples=CONTEXT_EXAMPLES,\n",
    "        batch_size=1,\n",
    "    )\n",
    "    action_results[model_key] = {k: v for k, v in metrics.items() if k not in {\"predictions\", \"targets\"}}\n",
    "    release_model(model_key)\n",
    "\n",
    "action_results_path = Path(\"2.3-result.json\")\n",
    "Utils.save_json(action_results, action_results_path)\n",
    "print(f\"Saved zero-shot action recognition results to {action_results_path}\")\n",
    "\n",
    "release_all_models()\n",
    "\n",
    "action_results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86ca3c70",
   "metadata": {},
   "source": [
    "2.4 plot the result of the evalutation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "80857f24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Plot saved to plots/2.4-action-metric-bars.png\n",
      "Plot saved to plots/2.4-action-heatmap.png\n",
      "Plot saved to plots/2.4-confusion-qwen3-0.6b.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_65491/2709129330.py:254: UserWarning: Tight layout not applied. The left and right margins cannot be made large enough to accommodate all Axes decorations.\n",
      "  plt.tight_layout()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Plot saved to plots/2.4-confusion-qwen3-4b.png\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'qwen3-0.6b': {'regex_matches': 0,\n",
       "  'strict_match_accuracy': 0.0,\n",
       "  'accuracy': 0.0,\n",
       "  'precision': 0.0,\n",
       "  'recall': 0.0,\n",
       "  'f1': 0.0,\n",
       "  'labels': ['irt|\\n|air|air|grass block|\\n|air|air|grass block|\\n|air|air|grass block|\\n\\n\\nFrame:\\n|air|grass block|dirt|\\n|air|grass block|dirt|\\n|air|grass block|dirt|\\n|air|air|dirt|\\n|air|air|dirt|\\n|air|air|dirt|\\n|air|air|grass block|\\n|air|air|grass block|\\n|air|air|grass block|\\n\\n\\nAction:\\nstraight: forward\\npan: left\\njump: jump\\n\\n\\nFrame:\\n|air|grass block|dirt|\\n|air|grass block|dirt|\\n|air|grass block|dirt|\\n|air|air|dirt|\\n|air|air|dirt|\\n|air|air|dirt|\\n|air|air|grass block|\\n|air|air|grass block|\\n|air|air|grass block|\\n\\n\\nFrame:\\n|air|grass block|dirt|\\n|air|grass block|dirt|\\n|air|grass block|dirt|\\n|air|air|dirt|\\n|air|air|dirt|\\n|air|air|dirt|\\n|air|air|grass',\n",
       "   'straight: forward\\npan: left\\njump: jump'],\n",
       "  'confusion_matrix': [[0, 0], [2, 0]],\n",
       "  'reconstruction_accuracy': 0.0,\n",
       "  'word2vec_cosine': 0.9999999403953552,\n",
       "  'word2vec_scores': [0.9999999403953552, 0.9999999403953552],\n",
       "  'model': 'qwen3-0.6b',\n",
       "  'num_samples': 2,\n",
       "  'num_context_examples': 5},\n",
       " 'qwen3-4b': {'regex_matches': 0,\n",
       "  'strict_match_accuracy': 0.0,\n",
       "  'accuracy': 0.0,\n",
       "  'precision': 0.0,\n",
       "  'recall': 0.0,\n",
       "  'f1': 0.0,\n",
       "  'labels': ['irt|\\n|air|air|grass block|\\n|air|air|grass block|\\n|air|air|grass block|\\n\\n\\nFrame:\\n|air|grass block|dirt|\\n|air|grass block|dirt|\\n|air|grass block|dirt|\\n|air|air|dirt|\\n|air|air|dirt|\\n|air|air|dirt|\\n|air|air|grass block|\\n|air|air|grass block|\\n|air|air|grass block|\\n\\n\\nAction:\\nstraight: forward\\npan: left\\njump: jump\\nOkay, I need to figure out the correct action based on the given frames. Let me look at the pattern. The frames repeat the same structure, and the actions are always \"straight: forward\", \"pan: left\", \"jump: jump\". Since the pattern alternates between Frame and Action, and the last frame provided is the same as the previous ones, the next action should be the third one in the sequence. The actions cycle through forward, left, jump. So after two actions (forward and left), the next should be jump. Therefore, the action is \"jump: jump\".\\nstraight: forward\\npan: left\\njump: jump\\njump: jump\\n\\nWait',\n",
       "   'straight: forward\\npan: left\\njump: jump'],\n",
       "  'confusion_matrix': [[0, 0], [2, 0]],\n",
       "  'reconstruction_accuracy': 0.0,\n",
       "  'word2vec_cosine': 0.9676053524017334,\n",
       "  'word2vec_scores': [0.9676053524017334, 0.9676053524017334],\n",
       "  'model': 'qwen3-4b',\n",
       "  'num_samples': 2,\n",
       "  'num_context_examples': 5}}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "plot_utils = PlotUtils()\n",
    "action_results = Utils.load_json(\"2.3-result.json\") or {}\n",
    "\n",
    "if not action_results:\n",
    "    print(\"No zero-shot action recognition results found. Run cell 2.3 first.\")\n",
    "else:\n",
    "    PlotUtils.plot_multi_metric_bar(\n",
    "        action_results,\n",
    "        metric_keys=[\"strict_match_accuracy\", \"word2vec_cosine\", \"f1\"],\n",
    "        metric_labels=[\"Strict Match Accuracy\", \"Word2Vec Cosine\", \"Macro F1\"],\n",
    "        title=\"Zero-Shot Action Recognition Metrics\",\n",
    "        save_path=\"plots/2.4-action-metric-bars.png\",\n",
    "        scales=[100.0, 100.0, 100.0],\n",
    "        ylabel=\"Score (%)\",\n",
    "        ylim=(0, 100),\n",
    "    )\n",
    "    PlotUtils.plot_metrics_heatmap(\n",
    "        action_results,\n",
    "        \"Zero-Shot Action Recognition Heatmap\",\n",
    "        \"plots/2.4-action-heatmap.png\",\n",
    "        metrics=[\"strict_match_accuracy\", \"word2vec_cosine\", \"precision\", \"recall\", \"f1\"],\n",
    "        metric_labels=[\n",
    "            \"Strict Match Accuracy (%)\",\n",
    "            \"Word2Vec Cosine (%)\",\n",
    "            \"Precision (%)\",\n",
    "            \"Recall (%)\",\n",
    "            \"Macro F1 (%)\",\n",
    "        ],\n",
    "        scales=[100.0, 100.0, 100.0, 100.0, 100.0],\n",
    "    )\n",
    "    for model_key, metrics in action_results.items():\n",
    "        conf = metrics.get(\"confusion_matrix\")\n",
    "        labels = metrics.get(\"labels\", [])\n",
    "        if conf and labels:\n",
    "            PlotUtils.plot_confusion_matrix(\n",
    "                np.array(conf),\n",
    "                labels,\n",
    "                f\"Action Recognition Confusion Matrix ({model_key})\",\n",
    "                f\"plots/2.4-confusion-{model_key}.png\",\n",
    "            )\n",
    "\n",
    "action_results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f26871a2",
   "metadata": {},
   "source": [
    "3.1 fine tune both model with lora method, task is next frame reconstraction, input x and y, output z, with loaded data train and stop in val and monitor via w&b, do not pass model parameter to w&b, keep them in local dir checkpoints with peroper naming and also keep a log in the dir logs. and create 3.1-training-metadata.json file in the working dir given the match betwen the run folder path under checkpoints and the run log path."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "246c5b56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Supervised LoRA training skipped. Set ENABLE_TRAINING = True to run fine-tuning.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "from pathlib import Path\n",
    "\n",
    "training_config = config.get_config()\n",
    "training_config[\"batch_size\"] = max(1, min(training_config[\"batch_size\"], len(SPLITS[\"train\"])))\n",
    "lora_config = {\n",
    "    \"r\": training_config[\"lora_r\"],\n",
    "    \"lora_alpha\": training_config[\"lora_alpha\"],\n",
    "    \"lora_dropout\": training_config[\"lora_dropout\"],\n",
    "}\n",
    "\n",
    "ENABLE_TRAINING = False\n",
    "TRAINING_MODELS = [\"qwen3-0.6b\", \"qwen3-4b\"]\n",
    "\n",
    "training_metadata = {}\n",
    "\n",
    "if ENABLE_TRAINING and SPLITS[\"train\"]:\n",
    "    for model_key in TRAINING_MODELS:\n",
    "        print(f\"Starting LoRA fine-tuning for {model_key}...\")\n",
    "        wrapper = get_model_wrapper(model_key, use_lora=True, lora_config=lora_config, force_reload=True)\n",
    "\n",
    "        train_loader = build_dataloader(\n",
    "            SPLITS[\"train\"],\n",
    "            wrapper.tokenizer,\n",
    "            \"frame_reconstruction\",\n",
    "            batch_size=training_config[\"batch_size\"],\n",
    "            shuffle=True,\n",
    "            context_examples=None,\n",
    "        )\n",
    "        val_loader = build_dataloader(\n",
    "            SPLITS[\"val\"],\n",
    "            wrapper.tokenizer,\n",
    "            \"frame_reconstruction\",\n",
    "            batch_size=1,\n",
    "            shuffle=False,\n",
    "            context_examples=None,\n",
    "        )\n",
    "\n",
    "        metadata = wrapper.train(\n",
    "            train_loader,\n",
    "            val_loader,\n",
    "            training_config,\n",
    "            task_name=f\"frame_reconstruction_{model_key}\",\n",
    "            use_wandb=WANDB_ENABLED,\n",
    "        )\n",
    "\n",
    "        metadata_path = Path(f\"3.1-training-metadata-{model_key}.json\")\n",
    "        Utils.save_json(metadata, metadata_path)\n",
    "        training_metadata[model_key] = metadata\n",
    "        release_model(model_key)\n",
    "        del wrapper\n",
    "else:\n",
    "    print(\"Supervised LoRA training skipped. Set ENABLE_TRAINING = True to run fine-tuning.\")\n",
    "\n",
    "release_all_models()\n",
    "\n",
    "training_metadata\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f1bd6cc",
   "metadata": {},
   "source": [
    "3.2 evaluate on test dataset, and save to 3.2-result.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8fc5ae36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model: models/Qwen3-0.6B\n",
      "LoRA adapter applied\n",
      "NOTE: Only LoRA adapter weights will be saved (saves disk space)\n",
      "Model loaded on cuda\n",
      "No fine-tuned weights found for qwen3-0.6b; skipping evaluation.\n",
      "Loading model: models/Qwen3-4B\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "185456ea3c9d47118f701afea17c4664",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LoRA adapter applied\n",
      "NOTE: Only LoRA adapter weights will be saved (saves disk space)\n",
      "Model loaded on cuda\n",
      "No fine-tuned weights found for qwen3-4b; skipping evaluation.\n",
      "No fine-tuned results to save.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "fine_tuned_results = {}\n",
    "for model_key in TRAINING_MODELS:\n",
    "    metadata_path = Path(f\"3.1-training-metadata-{model_key}.json\")\n",
    "    metadata = Utils.load_json(metadata_path)\n",
    "    if not metadata:\n",
    "        print(f\"No training metadata found for {model_key}; skipping.\")\n",
    "        continue\n",
    "\n",
    "    wrapper = get_model_wrapper(model_key, use_lora=True, lora_config=lora_config, force_reload=True)\n",
    "    checkpoint_dir = Path(metadata[\"checkpoint_dir\"])\n",
    "    adapter_path = checkpoint_dir / \"best_lora_adapter\"\n",
    "    model_path = checkpoint_dir / \"best_model.pt\"\n",
    "\n",
    "    if adapter_path.exists():\n",
    "        wrapper.load_checkpoint(str(adapter_path))\n",
    "    elif model_path.exists():\n",
    "        wrapper.load_checkpoint(str(model_path))\n",
    "    else:\n",
    "        print(f\"No fine-tuned weights found for {model_key}; skipping evaluation.\")\n",
    "        release_model(model_key)\n",
    "        del wrapper\n",
    "        continue\n",
    "\n",
    "    metrics = evaluate_wrapper(\n",
    "        wrapper,\n",
    "        model_key,\n",
    "        \"frame_reconstruction\",\n",
    "        SPLITS[\"test\"],\n",
    "        context_examples=None,\n",
    "    )\n",
    "    fine_tuned_results[model_key] = {k: v for k, v in metrics.items() if k not in {\"predictions\", \"targets\"}}\n",
    "    release_model(model_key)\n",
    "    del wrapper\n",
    "\n",
    "if fine_tuned_results:\n",
    "    Utils.save_json(fine_tuned_results, \"3.2-result.json\")\n",
    "    print(\"Saved fine-tuned evaluation results to 3.2-result.json\")\n",
    "else:\n",
    "    print(\"No fine-tuned results to save.\")\n",
    "\n",
    "release_all_models()\n",
    "\n",
    "fine_tuned_results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e86f2a8e",
   "metadata": {},
   "source": [
    "3.3 plot the evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b7398c62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Plot saved to plots/3.3-frame-reconstruction-accuracy.png\n",
      "Plot saved to plots/3.3-frame-strict-accuracy.png\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'zero_shot': {'qwen3-0.6b': {'regex_matches': 0,\n",
       "   'strict_match_accuracy': 0.0,\n",
       "   'accuracy': 0.055205047318611984,\n",
       "   'precision': 0.0,\n",
       "   'recall': 0.0,\n",
       "   'f1': 0.0,\n",
       "   'labels': [],\n",
       "   'confusion_matrix': [],\n",
       "   'reconstruction_accuracy': 0.055205047318611984,\n",
       "   'reconstruction_scores': [0.055205047318611984, 0.055205047318611984],\n",
       "   'word2vec_cosine': 0.0,\n",
       "   'model': 'qwen3-0.6b',\n",
       "   'num_samples': 2,\n",
       "   'num_context_examples': 5},\n",
       "  'qwen3-4b': {'regex_matches': 0,\n",
       "   'strict_match_accuracy': 0.0,\n",
       "   'accuracy': 0.07267221801665405,\n",
       "   'precision': 0.0,\n",
       "   'recall': 0.0,\n",
       "   'f1': 0.0,\n",
       "   'labels': [],\n",
       "   'confusion_matrix': [],\n",
       "   'reconstruction_accuracy': 0.07267221801665405,\n",
       "   'reconstruction_scores': [0.07267221801665405, 0.07267221801665405],\n",
       "   'word2vec_cosine': 0.0,\n",
       "   'model': 'qwen3-4b',\n",
       "   'num_samples': 2,\n",
       "   'num_context_examples': 5}},\n",
       " 'fine_tuned': {'qwen3-0.6b': {'regex_matches': 0,\n",
       "   'strict_match_accuracy': 0.0,\n",
       "   'accuracy': 0.06538461538461539,\n",
       "   'precision': 0.0,\n",
       "   'recall': 0.0,\n",
       "   'f1': 0.0,\n",
       "   'labels': [],\n",
       "   'confusion_matrix': [],\n",
       "   'reconstruction_accuracy': 0.06538461538461539,\n",
       "   'reconstruction_scores': [0.06538461538461539, 0.06538461538461539],\n",
       "   'word2vec_cosine': 0.0,\n",
       "   'model': 'qwen3-0.6b',\n",
       "   'num_samples': 2,\n",
       "   'num_context_examples': 0},\n",
       "  'qwen3-4b': {'regex_matches': 0,\n",
       "   'strict_match_accuracy': 0.0,\n",
       "   'accuracy': 0.016296296296296295,\n",
       "   'precision': 0.0,\n",
       "   'recall': 0.0,\n",
       "   'f1': 0.0,\n",
       "   'labels': [],\n",
       "   'confusion_matrix': [],\n",
       "   'reconstruction_accuracy': 0.016296296296296295,\n",
       "   'reconstruction_scores': [0.016296296296296295, 0.016296296296296295],\n",
       "   'word2vec_cosine': 0.0,\n",
       "   'model': 'qwen3-4b',\n",
       "   'num_samples': 2,\n",
       "   'num_context_examples': 0}}}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "plot_utils = PlotUtils()\n",
    "\n",
    "zero_shot_frame = Utils.load_json(\"2.1-result.json\") or {}\n",
    "fine_tuned_frame = Utils.load_json(\"3.2-result.json\") or {}\n",
    "\n",
    "method_results = {}\n",
    "if zero_shot_frame:\n",
    "    method_results[\"zero_shot\"] = zero_shot_frame\n",
    "if fine_tuned_frame:\n",
    "    method_results[\"fine_tuned\"] = fine_tuned_frame\n",
    "\n",
    "if len(method_results) >= 2:\n",
    "    PlotUtils.plot_method_metric_bar(\n",
    "        method_results,\n",
    "        metric_key=\"reconstruction_accuracy\",\n",
    "        title=\"Frame Reconstruction: Reconstruction Accuracy Comparison\",\n",
    "        save_path=\"plots/3.3-frame-reconstruction-accuracy.png\",\n",
    "        scale=100.0,\n",
    "        ylabel=\"Reconstruction Accuracy (%)\",\n",
    "        method_labels={\"zero_shot\": \"Zero-Shot\", \"fine_tuned\": \"LoRA Fine-Tuned\"},\n",
    "        metric_label=\"Reconstruction Accuracy (%)\",\n",
    "        ylim=(0, 100),\n",
    "    )\n",
    "    PlotUtils.plot_method_metric_bar(\n",
    "        method_results,\n",
    "        metric_key=\"strict_match_accuracy\",\n",
    "        title=\"Frame Reconstruction: Strict Match Accuracy Comparison\",\n",
    "        save_path=\"plots/3.3-frame-strict-accuracy.png\",\n",
    "        scale=100.0,\n",
    "        ylabel=\"Strict Match Accuracy (%)\",\n",
    "        method_labels={\"zero_shot\": \"Zero-Shot\", \"fine_tuned\": \"LoRA Fine-Tuned\"},\n",
    "        metric_label=\"Strict Match Accuracy (%)\",\n",
    "        ylim=(0, 100),\n",
    "    )\n",
    "else:\n",
    "    print(\"Need results from at least two methods to plot comparisons. Run zero-shot (2.1) and fine-tuned (3.2) evaluations.\")\n",
    "\n",
    "{\"zero_shot\": zero_shot_frame, \"fine_tuned\": fine_tuned_frame}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dba795fd",
   "metadata": {},
   "source": [
    "### 4.1 LoRA Fine-Tuning for Action Recognition\n",
    "\n",
    "Fine-tune both Qwen models on the action recognition task using LoRA. Each run stores checkpoints in `checkpoints/`, logs in `logs/`, and records training metadata to `4.1-training-metadata.json` for downstream evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cbf6e959",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting LoRA action recognition fine-tuning for qwen3-0.6b...\n",
      "Loading model: models/Qwen3-0.6B\n",
      "LoRA adapter applied\n",
      "NOTE: Only LoRA adapter weights will be saved (saves disk space)\n",
      "Model loaded on cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/3: 100%|██████████| 1/1 [00:00<00:00,  6.88it/s, loss=1.66]\n",
      "2025-10-16 00:00:55,867 - action_recognition_qwen3-0.6b - INFO - Epoch 1: train_loss=1.6580, val_loss=1.6582\n",
      "2025-10-16 00:00:55,912 - action_recognition_qwen3-0.6b - INFO - Best LoRA adapter saved with val_loss=1.6582\n",
      "Epoch 2/3: 100%|██████████| 1/1 [00:00<00:00, 10.65it/s, loss=1.66]\n",
      "2025-10-16 00:00:56,025 - action_recognition_qwen3-0.6b - INFO - Epoch 2: train_loss=1.6580, val_loss=1.6580\n",
      "2025-10-16 00:00:56,062 - action_recognition_qwen3-0.6b - INFO - Best LoRA adapter saved with val_loss=1.6580\n",
      "Epoch 3/3: 100%|██████████| 1/1 [00:00<00:00, 10.74it/s, loss=1.66]\n",
      "2025-10-16 00:00:56,175 - action_recognition_qwen3-0.6b - INFO - Epoch 3: train_loss=1.6579, val_loss=1.6575\n",
      "2025-10-16 00:00:56,209 - action_recognition_qwen3-0.6b - INFO - Best LoRA adapter saved with val_loss=1.6575\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting LoRA action recognition fine-tuning for qwen3-4b...\n",
      "Loading model: models/Qwen3-4B\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "52f9a595f60a41f18ddb1b22be65562a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LoRA adapter applied\n",
      "NOTE: Only LoRA adapter weights will be saved (saves disk space)\n",
      "Model loaded on cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/3: 100%|██████████| 1/1 [00:00<00:00,  2.64it/s, loss=1.68]\n",
      "2025-10-16 00:00:58,428 - action_recognition_qwen3-4b - INFO - Epoch 1: train_loss=1.6804, val_loss=1.6809\n",
      "2025-10-16 00:00:58,484 - action_recognition_qwen3-4b - INFO - Best LoRA adapter saved with val_loss=1.6809\n",
      "Epoch 2/3: 100%|██████████| 1/1 [00:00<00:00,  2.59it/s, loss=1.68]\n",
      "2025-10-16 00:00:58,925 - action_recognition_qwen3-4b - INFO - Epoch 2: train_loss=1.6804, val_loss=1.6809\n",
      "Epoch 3/3:   0%|          | 0/1 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 574.00 MiB. GPU 0 has a total capacity of 15.57 GiB of which 246.62 MiB is free. Including non-PyTorch memory, this process has 15.26 GiB memory in use. Of the allocated memory 13.85 GiB is allocated by PyTorch, and 1.12 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mOutOfMemoryError\u001b[39m                          Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[15]\u001b[39m\u001b[32m, line 40\u001b[39m\n\u001b[32m     23\u001b[39m train_loader = build_dataloader(\n\u001b[32m     24\u001b[39m     SPLITS[\u001b[33m\"\u001b[39m\u001b[33mtrain\u001b[39m\u001b[33m\"\u001b[39m],\n\u001b[32m     25\u001b[39m     wrapper.tokenizer,\n\u001b[32m   (...)\u001b[39m\u001b[32m     29\u001b[39m     context_examples=\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m     30\u001b[39m )\n\u001b[32m     31\u001b[39m val_loader = build_dataloader(\n\u001b[32m     32\u001b[39m     SPLITS[\u001b[33m\"\u001b[39m\u001b[33mval\u001b[39m\u001b[33m\"\u001b[39m],\n\u001b[32m     33\u001b[39m     wrapper.tokenizer,\n\u001b[32m   (...)\u001b[39m\u001b[32m     37\u001b[39m     context_examples=\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m     38\u001b[39m )\n\u001b[32m---> \u001b[39m\u001b[32m40\u001b[39m metadata = \u001b[43mwrapper\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     41\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     42\u001b[39m \u001b[43m    \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     43\u001b[39m \u001b[43m    \u001b[49m\u001b[43maction_training_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     44\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtask_name\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43mf\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43maction_recognition_\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mmodel_key\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     45\u001b[39m \u001b[43m    \u001b[49m\u001b[43muse_wandb\u001b[49m\u001b[43m=\u001b[49m\u001b[43mWANDB_ENABLED\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     46\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     47\u001b[39m metadata[\u001b[33m\"\u001b[39m\u001b[33mtask_type\u001b[39m\u001b[33m\"\u001b[39m] = \u001b[33m\"\u001b[39m\u001b[33maction_recognition\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     48\u001b[39m metadata[\u001b[33m\"\u001b[39m\u001b[33mconfig\u001b[39m\u001b[33m\"\u001b[39m] = {\n\u001b[32m     49\u001b[39m     key: action_training_config[key]\n\u001b[32m     50\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m [\n\u001b[32m   (...)\u001b[39m\u001b[32m     60\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m action_training_config\n\u001b[32m     61\u001b[39m }\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 145\u001b[39m, in \u001b[36mModelWrapper.train\u001b[39m\u001b[34m(self, train_loader, val_loader, config, task_name, use_wandb)\u001b[39m\n\u001b[32m    138\u001b[39m outputs = \u001b[38;5;28mself\u001b[39m.model(\n\u001b[32m    139\u001b[39m     input_ids=input_ids,\n\u001b[32m    140\u001b[39m     attention_mask=attention_mask,\n\u001b[32m    141\u001b[39m     labels=labels\n\u001b[32m    142\u001b[39m )\n\u001b[32m    144\u001b[39m loss = outputs.loss\n\u001b[32m--> \u001b[39m\u001b[32m145\u001b[39m \u001b[43mloss\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    147\u001b[39m torch.nn.utils.clip_grad_norm_(\u001b[38;5;28mself\u001b[39m.model.parameters(), config.get(\u001b[33m'\u001b[39m\u001b[33mmax_grad_norm\u001b[39m\u001b[33m'\u001b[39m, \u001b[32m1.0\u001b[39m))\n\u001b[32m    149\u001b[39m optimizer.step()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/workspace/nlp701-lab/project/llm/.venv/lib/python3.11/site-packages/torch/_tensor.py:625\u001b[39m, in \u001b[36mTensor.backward\u001b[39m\u001b[34m(self, gradient, retain_graph, create_graph, inputs)\u001b[39m\n\u001b[32m    615\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    616\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[32m    617\u001b[39m         Tensor.backward,\n\u001b[32m    618\u001b[39m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[32m   (...)\u001b[39m\u001b[32m    623\u001b[39m         inputs=inputs,\n\u001b[32m    624\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m625\u001b[39m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mautograd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    626\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43minputs\u001b[49m\n\u001b[32m    627\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/workspace/nlp701-lab/project/llm/.venv/lib/python3.11/site-packages/torch/autograd/__init__.py:354\u001b[39m, in \u001b[36mbackward\u001b[39m\u001b[34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[39m\n\u001b[32m    349\u001b[39m     retain_graph = create_graph\n\u001b[32m    351\u001b[39m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[32m    352\u001b[39m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[32m    353\u001b[39m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m354\u001b[39m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    355\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    356\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    357\u001b[39m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    358\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    359\u001b[39m \u001b[43m    \u001b[49m\u001b[43minputs_tuple\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    360\u001b[39m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    361\u001b[39m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    362\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/workspace/nlp701-lab/project/llm/.venv/lib/python3.11/site-packages/torch/autograd/graph.py:841\u001b[39m, in \u001b[36m_engine_run_backward\u001b[39m\u001b[34m(t_outputs, *args, **kwargs)\u001b[39m\n\u001b[32m    839\u001b[39m     unregister_hooks = _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[32m    840\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m841\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_execution_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[32m    842\u001b[39m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    843\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[32m    844\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    845\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[31mOutOfMemoryError\u001b[39m: CUDA out of memory. Tried to allocate 574.00 MiB. GPU 0 has a total capacity of 15.57 GiB of which 246.62 MiB is free. Including non-PyTorch memory, this process has 15.26 GiB memory in use. Of the allocated memory 13.85 GiB is allocated by PyTorch, and 1.12 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "action_training_config = config.get_config()\n",
    "action_training_config[\"batch_size\"] = max(1, min(action_training_config[\"batch_size\"], len(SPLITS[\"train\"])))\n",
    "action_lora_config = {\n",
    "    \"r\": action_training_config[\"lora_r\"],\n",
    "    \"lora_alpha\": action_training_config[\"lora_alpha\"],\n",
    "    \"lora_dropout\": action_training_config[\"lora_dropout\"],\n",
    "}\n",
    "\n",
    "ENABLE_ACTION_TRAINING = True\n",
    "ACTION_TRAINING_MODELS = [\"qwen3-0.6b\", \"qwen3-4b\"]\n",
    "\n",
    "action_training_metadata = {}\n",
    "\n",
    "if not SPLITS[\"train\"]:\n",
    "    print(\"No training samples available for action recognition. Populate SPLITS['train'] before running fine-tuning.\")\n",
    "elif ENABLE_ACTION_TRAINING:\n",
    "    for model_key in ACTION_TRAINING_MODELS:\n",
    "        print(f\"Starting LoRA action recognition fine-tuning for {model_key}...\")\n",
    "        wrapper = get_model_wrapper(model_key, use_lora=True, lora_config=action_lora_config, force_reload=True)\n",
    "\n",
    "        train_loader = build_dataloader(\n",
    "            SPLITS[\"train\"],\n",
    "            wrapper.tokenizer,\n",
    "            \"action_recognition\",\n",
    "            batch_size=action_training_config[\"batch_size\"],\n",
    "            shuffle=True,\n",
    "            context_examples=None,\n",
    "        )\n",
    "        val_loader = build_dataloader(\n",
    "            SPLITS[\"val\"],\n",
    "            wrapper.tokenizer,\n",
    "            \"action_recognition\",\n",
    "            batch_size=1,\n",
    "            shuffle=False,\n",
    "            context_examples=None,\n",
    "        )\n",
    "\n",
    "        metadata = wrapper.train(\n",
    "            train_loader,\n",
    "            val_loader,\n",
    "            action_training_config,\n",
    "            task_name=f\"action_recognition_{model_key}\",\n",
    "            use_wandb=WANDB_ENABLED,\n",
    "        )\n",
    "        metadata[\"task_type\"] = \"action_recognition\"\n",
    "        metadata[\"config\"] = {\n",
    "            key: action_training_config[key]\n",
    "            for key in [\n",
    "                \"learning_rate\",\n",
    "                \"num_epochs\",\n",
    "                \"batch_size\",\n",
    "                \"lora_r\",\n",
    "                \"lora_alpha\",\n",
    "                \"lora_dropout\",\n",
    "                \"warmup_steps\",\n",
    "                \"max_grad_norm\",\n",
    "            ]\n",
    "            if key in action_training_config\n",
    "        }\n",
    "\n",
    "        metadata_path = Path(f\"4.1-training-metadata-{model_key}.json\")\n",
    "        Utils.save_json(metadata, metadata_path)\n",
    "        action_training_metadata[model_key] = metadata\n",
    "        release_model(model_key)\n",
    "        del wrapper\n",
    "\n",
    "    Utils.save_json(action_training_metadata, \"4.1-training-metadata.json\")\n",
    "    print(\"Saved aggregated training metadata to 4.1-training-metadata.json\")\n",
    "else:\n",
    "    print(\"Action recognition LoRA training skipped. Set ENABLE_ACTION_TRAINING = True to run fine-tuning.\")\n",
    "\n",
    "release_all_models()\n",
    "\n",
    "action_training_metadata\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b1b8288",
   "metadata": {},
   "source": [
    "### 4.2 Evaluate Fine-Tuned Action Recognition\n",
    "\n",
    "Load the best adapters from Section 4.1, run inference on the test split, and persist aggregated metrics to `4.2-result.json`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14cc34b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model: models/Qwen3-0.6B\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/shirox/workspace/nlp701-lab/project/llm/.venv/lib/python3.11/site-packages/peft/tuners/tuners_utils.py:196: UserWarning: Already found a `peft_config` attribute in the model. This will lead to having multiple adapters in the model. Make sure to know what you are doing!\n",
      "  warnings.warn(\n",
      "/home/shirox/workspace/nlp701-lab/project/llm/.venv/lib/python3.11/site-packages/peft/peft_model.py:585: UserWarning: Found missing adapter keys while loading the checkpoint: ['base_model.model.base_model.model.model.layers.0.self_attn.q_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.0.self_attn.q_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.0.self_attn.v_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.0.self_attn.v_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.1.self_attn.q_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.1.self_attn.q_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.1.self_attn.v_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.1.self_attn.v_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.2.self_attn.q_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.2.self_attn.q_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.2.self_attn.v_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.2.self_attn.v_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.3.self_attn.q_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.3.self_attn.q_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.3.self_attn.v_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.3.self_attn.v_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.4.self_attn.q_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.4.self_attn.q_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.4.self_attn.v_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.4.self_attn.v_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.5.self_attn.q_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.5.self_attn.q_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.5.self_attn.v_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.5.self_attn.v_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.6.self_attn.q_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.6.self_attn.q_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.6.self_attn.v_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.6.self_attn.v_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.7.self_attn.q_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.7.self_attn.q_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.7.self_attn.v_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.7.self_attn.v_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.8.self_attn.q_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.8.self_attn.q_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.8.self_attn.v_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.8.self_attn.v_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.9.self_attn.q_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.9.self_attn.q_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.9.self_attn.v_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.9.self_attn.v_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.10.self_attn.q_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.10.self_attn.q_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.10.self_attn.v_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.10.self_attn.v_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.11.self_attn.q_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.11.self_attn.q_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.11.self_attn.v_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.11.self_attn.v_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.12.self_attn.q_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.12.self_attn.q_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.12.self_attn.v_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.12.self_attn.v_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.13.self_attn.q_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.13.self_attn.q_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.13.self_attn.v_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.13.self_attn.v_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.14.self_attn.q_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.14.self_attn.q_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.14.self_attn.v_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.14.self_attn.v_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.15.self_attn.q_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.15.self_attn.q_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.15.self_attn.v_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.15.self_attn.v_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.16.self_attn.q_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.16.self_attn.q_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.16.self_attn.v_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.16.self_attn.v_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.17.self_attn.q_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.17.self_attn.q_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.17.self_attn.v_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.17.self_attn.v_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.18.self_attn.q_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.18.self_attn.q_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.18.self_attn.v_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.18.self_attn.v_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.19.self_attn.q_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.19.self_attn.q_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.19.self_attn.v_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.19.self_attn.v_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.20.self_attn.q_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.20.self_attn.q_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.20.self_attn.v_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.20.self_attn.v_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.21.self_attn.q_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.21.self_attn.q_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.21.self_attn.v_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.21.self_attn.v_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.22.self_attn.q_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.22.self_attn.q_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.22.self_attn.v_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.22.self_attn.v_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.23.self_attn.q_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.23.self_attn.q_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.23.self_attn.v_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.23.self_attn.v_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.24.self_attn.q_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.24.self_attn.q_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.24.self_attn.v_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.24.self_attn.v_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.25.self_attn.q_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.25.self_attn.q_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.25.self_attn.v_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.25.self_attn.v_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.26.self_attn.q_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.26.self_attn.q_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.26.self_attn.v_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.26.self_attn.v_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.27.self_attn.q_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.27.self_attn.q_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.27.self_attn.v_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.27.self_attn.v_proj.lora_B.default.weight'].\n",
      "  warnings.warn(warn_message)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LoRA adapter applied\n",
      "NOTE: Only LoRA adapter weights will be saved (saves disk space)\n",
      "Model loaded on cuda\n",
      "LoRA adapter loaded from checkpoints/action_recognition_qwen3-0.6b_Qwen3-0.6B_20251015_235814/best_lora_adapter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 2/2 [00:06<00:00,  3.42s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved action recognition fine-tuned evaluation results to 4.2-result.json\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'qwen3-0.6b': {'regex_matches': 0,\n",
       "  'strict_match_accuracy': 0.0,\n",
       "  'accuracy': 0.0,\n",
       "  'precision': 0.0,\n",
       "  'recall': 0.0,\n",
       "  'f1': 0.0,\n",
       "  'labels': ['\\nturn: left\\nturn: right\\nAction: turn: left\\nAction: turn: right\\nAction: jump: jump\\nAction: straight: forward\\nAction: pan: left\\nAction: pan: right\\nAction: jump: jump\\nAction: turn: left\\nAction: turn: right\\nAction: turn: left\\nAction: turn: right\\nAction: turn: left\\nAction: turn: right\\nAction: turn: left\\nAction: turn: right\\nAction: turn: left\\nAction: turn: right\\nAction: turn: left\\nAction: turn: right\\nAction: turn: left\\nAction: turn: right\\nAction: turn: left\\nAction: turn: right\\nAction: turn: left\\nAction: turn: right\\nAction: turn: left\\nAction: turn: right\\nAction: turn: left\\nAction: turn: right\\nAction: turn: left\\nAction: turn: right\\nAction: turn: left\\nAction: turn: right\\nAction: turn: left\\nAction: turn: right\\nAction: turn: left\\nAction: turn: right\\nAction: turn: left\\nAction: turn: right\\nAction: turn: left\\nAction: turn: right\\nAction',\n",
       "   'straight: forward\\npan: left\\njump: jump'],\n",
       "  'confusion_matrix': [[0, 0], [2, 0]],\n",
       "  'reconstruction_accuracy': 0.0,\n",
       "  'word2vec_cosine': 0.6150259971618652,\n",
       "  'word2vec_scores': [0.6150259971618652, 0.6150259971618652],\n",
       "  'model': 'qwen3-0.6b',\n",
       "  'num_samples': 2,\n",
       "  'num_context_examples': 0}}"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "action_finetuned_results = {}\n",
    "metadata_index = Utils.load_json(\"4.1-training-metadata.json\") or {}\n",
    "\n",
    "if not metadata_index:\n",
    "    print(\"No action recognition training metadata found. Run cell 4.1 first.\")\n",
    "elif not SPLITS[\"test\"]:\n",
    "    print(\"No test samples available for action recognition evaluation. Populate SPLITS['test'] before running evaluation.\")\n",
    "else:\n",
    "    for model_key, metadata in metadata_index.items():\n",
    "        checkpoint_dir = Path(metadata.get(\"checkpoint_dir\", \"\"))\n",
    "        if not checkpoint_dir.exists():\n",
    "            print(f\"Checkpoint directory {checkpoint_dir} not found for {model_key}; skipping.\")\n",
    "            continue\n",
    "\n",
    "        wrapper = get_model_wrapper(model_key, use_lora=True, lora_config=action_lora_config, force_reload=True)\n",
    "\n",
    "        adapter_path = checkpoint_dir / \"best_lora_adapter\"\n",
    "        model_path = checkpoint_dir / \"best_model.pt\"\n",
    "\n",
    "        if adapter_path.exists():\n",
    "            wrapper.load_checkpoint(str(adapter_path))\n",
    "        elif model_path.exists():\n",
    "            wrapper.load_checkpoint(str(model_path))\n",
    "        else:\n",
    "            print(f\"No fine-tuned weights found for {model_key}; skipping evaluation.\")\n",
    "            release_model(model_key)\n",
    "            del wrapper\n",
    "            continue\n",
    "\n",
    "        metrics = evaluate_wrapper(\n",
    "            wrapper,\n",
    "            model_key,\n",
    "            \"action_recognition\",\n",
    "            SPLITS[\"test\"],\n",
    "            context_examples=None,\n",
    "        )\n",
    "        action_finetuned_results[model_key] = {\n",
    "            k: v for k, v in metrics.items() if k not in {\"predictions\", \"targets\"}\n",
    "        }\n",
    "        release_model(model_key)\n",
    "        del wrapper\n",
    "\n",
    "if action_finetuned_results:\n",
    "    Utils.save_json(action_finetuned_results, \"4.2-result.json\")\n",
    "    print(\"Saved action recognition fine-tuned evaluation results to 4.2-result.json\")\n",
    "else:\n",
    "    print(\"No fine-tuned action recognition results to save.\")\n",
    "\n",
    "release_all_models()\n",
    "\n",
    "action_finetuned_results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84502e15",
   "metadata": {},
   "source": [
    "### 4.3 Plot Action Recognition Comparison\n",
    "\n",
    "Compare zero-shot and LoRA fine-tuned performance using bar charts saved under `plots/`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fce88003",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Plot saved to plots/4.3-action-strict-accuracy.png\n",
      "Plot saved to plots/4.3-action-macro-f1.png\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'zero_shot': {'qwen3-0.6b': {'regex_matches': 0,\n",
       "   'strict_match_accuracy': 0.0,\n",
       "   'accuracy': 0.0,\n",
       "   'precision': 0.0,\n",
       "   'recall': 0.0,\n",
       "   'f1': 0.0,\n",
       "   'labels': ['irt|\\n|air|air|grass block|\\n|air|air|grass block|\\n|air|air|grass block|\\n\\n\\nFrame:\\n|air|grass block|dirt|\\n|air|grass block|dirt|\\n|air|grass block|dirt|\\n|air|air|dirt|\\n|air|air|dirt|\\n|air|air|dirt|\\n|air|air|grass block|\\n|air|air|grass block|\\n|air|air|grass block|\\n\\n\\nAction:\\nstraight: forward\\npan: left\\njump: jump\\n\\n\\nFrame:\\n|air|grass block|dirt|\\n|air|grass block|dirt|\\n|air|grass block|dirt|\\n|air|air|dirt|\\n|air|air|dirt|\\n|air|air|dirt|\\n|air|air|grass block|\\n|air|air|grass block|\\n|air|air|grass block|\\n\\n\\nFrame:\\n|air|grass block|dirt|\\n|air|grass block|dirt|\\n|air|grass block|dirt|\\n|air|air|dirt|\\n|air|air|dirt|\\n|air|air|dirt|\\n|air|air|grass',\n",
       "    'straight: forward\\npan: left\\njump: jump'],\n",
       "   'confusion_matrix': [[0, 0], [2, 0]],\n",
       "   'reconstruction_accuracy': 0.0,\n",
       "   'word2vec_cosine': 0.9999999403953552,\n",
       "   'word2vec_scores': [0.9999999403953552, 0.9999999403953552],\n",
       "   'model': 'qwen3-0.6b',\n",
       "   'num_samples': 2,\n",
       "   'num_context_examples': 5},\n",
       "  'qwen3-4b': {'regex_matches': 0,\n",
       "   'strict_match_accuracy': 0.0,\n",
       "   'accuracy': 0.0,\n",
       "   'precision': 0.0,\n",
       "   'recall': 0.0,\n",
       "   'f1': 0.0,\n",
       "   'labels': ['irt|\\n|air|air|grass block|\\n|air|air|grass block|\\n|air|air|grass block|\\n\\n\\nFrame:\\n|air|grass block|dirt|\\n|air|grass block|dirt|\\n|air|grass block|dirt|\\n|air|air|dirt|\\n|air|air|dirt|\\n|air|air|dirt|\\n|air|air|grass block|\\n|air|air|grass block|\\n|air|air|grass block|\\n\\n\\nAction:\\nstraight: forward\\npan: left\\njump: jump\\nOkay, I need to figure out the correct action based on the given frames. Let me look at the pattern. The frames repeat the same structure, and the actions are always \"straight: forward\", \"pan: left\", \"jump: jump\". Since the pattern alternates between Frame and Action, and the last frame provided is the same as the previous ones, the next action should be the third one in the sequence. The actions cycle through forward, left, jump. So after two actions (forward and left), the next should be jump. Therefore, the action is \"jump: jump\".\\nstraight: forward\\npan: left\\njump: jump\\njump: jump\\n\\nWait',\n",
       "    'straight: forward\\npan: left\\njump: jump'],\n",
       "   'confusion_matrix': [[0, 0], [2, 0]],\n",
       "   'reconstruction_accuracy': 0.0,\n",
       "   'word2vec_cosine': 0.9676053524017334,\n",
       "   'word2vec_scores': [0.9676053524017334, 0.9676053524017334],\n",
       "   'model': 'qwen3-4b',\n",
       "   'num_samples': 2,\n",
       "   'num_context_examples': 5}},\n",
       " 'fine_tuned': {'qwen3-0.6b': {'regex_matches': 0,\n",
       "   'strict_match_accuracy': 0.0,\n",
       "   'accuracy': 0.0,\n",
       "   'precision': 0.0,\n",
       "   'recall': 0.0,\n",
       "   'f1': 0.0,\n",
       "   'labels': ['\\nturn: left\\nturn: right\\nAction: turn: left\\nAction: turn: right\\nAction: jump: jump\\nAction: straight: forward\\nAction: pan: left\\nAction: pan: right\\nAction: jump: jump\\nAction: turn: left\\nAction: turn: right\\nAction: turn: left\\nAction: turn: right\\nAction: turn: left\\nAction: turn: right\\nAction: turn: left\\nAction: turn: right\\nAction: turn: left\\nAction: turn: right\\nAction: turn: left\\nAction: turn: right\\nAction: turn: left\\nAction: turn: right\\nAction: turn: left\\nAction: turn: right\\nAction: turn: left\\nAction: turn: right\\nAction: turn: left\\nAction: turn: right\\nAction: turn: left\\nAction: turn: right\\nAction: turn: left\\nAction: turn: right\\nAction: turn: left\\nAction: turn: right\\nAction: turn: left\\nAction: turn: right\\nAction: turn: left\\nAction: turn: right\\nAction: turn: left\\nAction: turn: right\\nAction: turn: left\\nAction: turn: right\\nAction',\n",
       "    'straight: forward\\npan: left\\njump: jump'],\n",
       "   'confusion_matrix': [[0, 0], [2, 0]],\n",
       "   'reconstruction_accuracy': 0.0,\n",
       "   'word2vec_cosine': 0.6150259971618652,\n",
       "   'word2vec_scores': [0.6150259971618652, 0.6150259971618652],\n",
       "   'model': 'qwen3-0.6b',\n",
       "   'num_samples': 2,\n",
       "   'num_context_examples': 0}}}"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "plot_utils = PlotUtils()\n",
    "\n",
    "zero_shot_action = Utils.load_json(\"2.3-result.json\") or {}\n",
    "fine_tuned_action = Utils.load_json(\"4.2-result.json\") or {}\n",
    "\n",
    "method_results = {}\n",
    "if zero_shot_action:\n",
    "    method_results[\"zero_shot\"] = zero_shot_action\n",
    "if fine_tuned_action:\n",
    "    method_results[\"fine_tuned\"] = fine_tuned_action\n",
    "\n",
    "if len(method_results) >= 2:\n",
    "    PlotUtils.plot_method_metric_bar(\n",
    "        method_results,\n",
    "        metric_key=\"strict_match_accuracy\",\n",
    "        title=\"Action Recognition: Strict Match Accuracy Comparison\",\n",
    "        save_path=\"plots/4.3-action-strict-accuracy.png\",\n",
    "        scale=100.0,\n",
    "        ylabel=\"Strict Match Accuracy (%)\",\n",
    "        method_labels={\"zero_shot\": \"Zero-Shot\", \"fine_tuned\": \"LoRA Fine-Tuned\"},\n",
    "        metric_label=\"Strict Match Accuracy (%)\",\n",
    "        ylim=(0, 100),\n",
    "    )\n",
    "    PlotUtils.plot_method_metric_bar(\n",
    "        method_results,\n",
    "        metric_key=\"f1\",\n",
    "        title=\"Action Recognition: Macro F1 Comparison\",\n",
    "        save_path=\"plots/4.3-action-macro-f1.png\",\n",
    "        scale=100.0,\n",
    "        ylabel=\"Macro F1 (%)\",\n",
    "        method_labels={\"zero_shot\": \"Zero-Shot\", \"fine_tuned\": \"LoRA Fine-Tuned\"},\n",
    "        metric_label=\"Macro F1 (%)\",\n",
    "        ylim=(0, 100),\n",
    "    )\n",
    "else:\n",
    "    print(\"Need zero-shot and fine-tuned results to plot comparisons. Run cells 2.3 and 4.2.\")\n",
    "\n",
    "{\"zero_shot\": zero_shot_action, \"fine_tuned\": fine_tuned_action}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
