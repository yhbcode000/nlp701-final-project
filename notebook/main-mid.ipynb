{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f871a58d",
   "metadata": {},
   "source": [
    "## 0. Infrastructure Setup\n",
    "\n",
    "### 0.1 Utils Module\n",
    "All helpful methods including validate local path, local logging, serialise and deserialise json file, read and write files, create and delete path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "rptw3m85tba",
   "metadata": {},
   "source": [
    "# Minecraft Voxel World LLM Training\n",
    "\n",
    "## Project Overview\n",
    "This notebook implements a complete pipeline for training Large Language Models (LLMs) on Minecraft voxel-based sequential prediction tasks:\n",
    "- **Frame Prediction**: Given current voxel state + action → predict next voxel state\n",
    "- **Action Recognition**: Given current voxel state + next voxel state → predict action taken\n",
    "\n",
    "## Dataset\n",
    "- **Source**: Minecraft gameplay data with 3D voxel representations\n",
    "- **Format**: Sequential .npy files containing voxel grids and actions\n",
    "- **Layout**: Flattened episodes at `datasets/minecraft/data/creative:{episode}/000123.npy` (one .npy per frame, zero-padded and lexicographically ordered)\n",
    "- **Structure**: Each frame contains a 3×3×3 block grid and action vector\n",
    "\n",
    "## Models\n",
    "- **Qwen 3 0.6B**: Small-scale model for efficient training\n",
    "- **Qwen 3 4B**: Larger model for improved performance\n",
    "\n",
    "## Methods\n",
    "- In-context learning (few-shot prompting with training examples)\n",
    "- Supervised fine-tuning with LoRA for frame reconstruction\n",
    "- Supervised fine-tuning with LoRA for action recognition\n",
    "\n",
    "---\n",
    "\n",
    "## Table of Contents\n",
    "\n",
    "### 0. Infrastructure Setup\n",
    "- **0.1** Utils Module - File I/O, logging, JSON operations\n",
    "- **0.2** Model Wrapper Class - Training, evaluation, checkpoint management\n",
    "- **0.3** Plot Evaluation Class - Conference-quality visualizations\n",
    "- **0.4** Hyperparameter Configuration - Grid search support\n",
    "\n",
    "### 1. Setup\n",
    "- **1.1** Load Models - Qwen 3 0.6B and 4B configuration\n",
    "- **1.2** Load Minecraft Dataset - Sequential voxel frames with actions\n",
    "- **1.3** Split Data - Train/val/test split (70%/15%/15%)\n",
    "\n",
    "### 2. In-Context Learning Evaluation\n",
    "- **2.1** Frame Reconstruction - Input: x+y, Output: z (with 3 training examples as context)\n",
    "- **2.2** Frame Reconstruction Plots - Visualization of results\n",
    "- **2.3** Action Recognition - Input: x+z, Output: y (with 3 training examples as context)\n",
    "- **2.4** Action Recognition Plots - Visualization of results\n",
    "\n",
    "### 3. Supervised Fine-Tuning (LoRA) for Frame Reconstruction \n",
    "- **3.1** Fine-tune Frame Reconstruction - LoRA adaptation with W&B monitoring\n",
    "- **3.2** Evaluate Fine-tuned Models - Test set performance\n",
    "- **3.3** Plot Fine-tuning Results - Compare in-context vs fine-tuned\n",
    "\n",
    "### 4. Supervised Fine-Tuning (LoRA) for Action Recognition\n",
    "- **4.1** Fine-tune Action Recognition - Train LoRA adapters to predict discrete actions\n",
    "- **4.2** Evaluate Action Recognition - Test set metrics and JSON export\n",
    "- **4.3** Plot Action Recognition Comparison - Bar charts versus zero-shot baseline\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d1533a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils_module import Utils"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b16dbc7c",
   "metadata": {},
   "source": [
    "### 0.2 Model Wrapper Class\n",
    "Including loading with name method, train with dataloaders method, and evaluate method.\n",
    "\n",
    "With loaded data train and stop in val and monitor via W&B. Do not pass model parameters to W&B. Keep them in local dir `checkpoints/` with proper naming and also keep a log in the dir `logs/`.  Create a JSON file with proper name of task in the working dir given the match between the run folder path under checkpoints and the run log path.\n",
    "\n",
    "The checkpoint resume from latest feature should be implemented - we do not want to train repeatedly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf7438fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from model_wrapper import ModelWrapper"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b776ad8b",
   "metadata": {},
   "source": [
    "### 0.3 Plot Evaluation Class\n",
    "Including all methods we need to plot conference-level paper quality plots."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67993775",
   "metadata": {},
   "outputs": [],
   "source": [
    "from plot_utils import PlotUtils"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc4788cc",
   "metadata": {},
   "source": [
    "### 0.4 Hyperparameter Configuration  \n",
    "Define all configurable hyperparameters and provide grid search method.\n",
    "\n",
    "Keep a local JSON called `grid-search-record.json` to save past running results. Each time we run the whole notebook, if we enable grid search, we have to read the JSON file and continue to the next grid search values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43883a74",
   "metadata": {},
   "outputs": [],
   "source": [
    "from hyperparameter_config import HyperparameterConfig\n",
    "\n",
    "config = HyperparameterConfig({\n",
    "    \"learning_rate\": 5e-5,\n",
    "    \"num_epochs\": 3,\n",
    "    \"batch_size\": 32,\n",
    "    \"max_length\": 1024,\n",
    "    \"lora_r\": 8,\n",
    "    \"lora_alpha\": 32,\n",
    "    \"lora_dropout\": 0.1,\n",
    "    \"warmup_steps\": 100,\n",
    "    \"max_grad_norm\": 1.0,\n",
    "    \"wandb_project\": \"minecraft-llm\",\n",
    "})\n",
    "config.print_config()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c73327ca",
   "metadata": {},
   "source": [
    "1.1 load model via transformers, we pick Qwen3-0.6B and Qwen3-4B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a43700f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from model_registry import (\n",
    "    MODEL_PATHS,\n",
    "    DEVICE,\n",
    "    WANDB_ENABLED,\n",
    "    get_model_wrapper,\n",
    "    release_model,\n",
    "    release_all_models,\n",
    ")\n",
    "\n",
    "print(f\"Available models: {list(MODEL_PATHS.keys())}\")\n",
    "print(f\"Using device: {DEVICE}\")\n",
    "print(\"Call get_model_wrapper('qwen3-0.6b') to load a model when needed.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "142b504e",
   "metadata": {},
   "source": [
    "1.2 load custom data from local datasets dir, the data with 3 types of data, \n",
    "\n",
    "- x : current frame in ascii art, \n",
    "- y: current action token, \n",
    "- z: next frame ascii art, \n",
    "\n",
    "all in plain text format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "706c7bbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from dataset_loader import load_full_dataset, preview_dataset_example\n",
    "\n",
    "DATA_DIR = Path(\"datasets/minecraft/data\")\n",
    "full_dataset, TOTAL_PAIRS, UNIQUE_ACTIONS = load_full_dataset(config, DATA_DIR)\n",
    "\n",
    "preview_dataset_example(full_dataset, 0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac3e0933",
   "metadata": {},
   "source": [
    "1.3 split loaded data to all, train, val and test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5a6280e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataset_utils import (\n",
    "    compute_dataset_splits,\n",
    "    build_dataloader,\n",
    "    build_action_embedder,\n",
    ")\n",
    "\n",
    "TRAIN_SUBSET_FRACTION = 0.1\n",
    "SPLITS, SELECTED_INDICES = compute_dataset_splits(full_dataset, subset_fraction=TRAIN_SUBSET_FRACTION)\n",
    "\n",
    "INFERENCE_SPLITS, _ = compute_dataset_splits(full_dataset, subset_fraction=0.05)\n",
    "INFERENCE_TEST_INDICES = INFERENCE_SPLITS['test']\n",
    "\n",
    "TOTAL_SAMPLES = len(SELECTED_INDICES)\n",
    "MAX_NEW_TOKENS = 512\n",
    "ACTION_EMBEDDER = build_action_embedder()\n",
    "\n",
    "print(\"Dataset split sizes:\", {split: len(idxs) for split, idxs in SPLITS.items()})\n",
    "print(\"Inference test split size:\", len(INFERENCE_TEST_INDICES))\n",
    "print(f\"History length per sample: {full_dataset.history_length}. Prompt context disabled (using loaded history).\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f4f2bbb",
   "metadata": {},
   "source": [
    "2.1 evaluate both models using history-only inputs for frame reconstruction: h → predict z, save to 2.1-result.json\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bcb67ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "frame_results_path = Path(\"2.1-result.json\")\n",
    "frame_raw_path = Path(\"2.1-raw.json\")\n",
    "\n",
    "if frame_results_path.exists() and frame_raw_path.exists():\n",
    "    frame_results = Utils.load_json(frame_results_path) or {}\n",
    "    frame_raw_outputs = Utils.load_json(frame_raw_path) or {}\n",
    "    print(f\"Loaded cached frame reconstruction results from {frame_results_path}\")\n",
    "    print(f\"Loaded cached raw outputs from {frame_raw_path}\")\n",
    "else:\n",
    "    test_indices = INFERENCE_TEST_INDICES if 'INFERENCE_TEST_INDICES' in globals() else SPLITS['test']\n",
    "\n",
    "    print(\n",
    "        f\"Evaluating frame reconstruction on {len(test_indices)} samples using history length {full_dataset.history_length}.\"\n",
    "    )\n",
    "\n",
    "    frame_results = {}\n",
    "    frame_raw_outputs = {}\n",
    "    for model_key in MODEL_PATHS:\n",
    "        wrapper = get_model_wrapper(model_key)\n",
    "        metrics = wrapper.evaluate_task(\n",
    "            full_dataset,\n",
    "            test_indices,\n",
    "            task_type=\"frame_reconstruction\",\n",
    "            model_key=model_key,\n",
    "            batch_size=1,\n",
    "            max_new_tokens=MAX_NEW_TOKENS,\n",
    "        )\n",
    "\n",
    "        predictions = metrics.get(\"predictions\", [])\n",
    "        targets = metrics.get(\"targets\", [])\n",
    "        raw_records = []\n",
    "        if len(predictions) != len(test_indices):\n",
    "            print(\n",
    "                f\"Warning: prediction count {len(predictions)} does not match test indices {len(test_indices)} for {model_key}.\"\n",
    "            )\n",
    "        for idx, pred, target in zip(test_indices, predictions, targets):\n",
    "            pair = full_dataset.data_pairs[int(idx)]\n",
    "            history = pair.get(\"history_reconstruction\") or pair.get(\"history_action\") or pair.get(\"x\")\n",
    "            raw_records.append(\n",
    "                {\n",
    "                    \"index\": int(idx),\n",
    "                    \"episode\": pair.get(\"episode\"),\n",
    "                    \"history\": history,\n",
    "                    \"z_label\": target,\n",
    "                    \"z_prediction\": pred,\n",
    "                }\n",
    "            )\n",
    "        frame_raw_outputs[model_key] = raw_records\n",
    "\n",
    "        frame_results[model_key] = {k: v for k, v in metrics.items() if k not in {\"predictions\", \"targets\"}}\n",
    "        release_model(model_key)\n",
    "\n",
    "    Utils.save_json(frame_results, frame_results_path)\n",
    "    Utils.save_json(frame_raw_outputs, frame_raw_path)\n",
    "    print(f\"Saved zero-shot frame reconstruction summary to {frame_results_path}\")\n",
    "    print(f\"Saved frame reconstruction raw outputs to {frame_raw_path}\")\n",
    "\n",
    "    release_all_models()\n",
    "\n",
    "frame_results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b314d905",
   "metadata": {},
   "source": [
    "2.2 update zero-shot frame reconstruction plots (h → z)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dcffcde",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "plot_utils = PlotUtils()\n",
    "frame_results = Utils.load_json(\"2.1-result.json\") or {}\n",
    "\n",
    "if not frame_results:\n",
    "    print(\"No zero-shot frame reconstruction results found. Run cell 2.1 first.\")\n",
    "else:\n",
    "    PlotUtils.plot_multi_metric_bar(\n",
    "        frame_results,\n",
    "        metric_keys=[\"strict_match_accuracy\", \"reconstruction_accuracy\"],\n",
    "        metric_labels=[\"Strict Match Accuracy\", \"Reconstruction Accuracy\"],\n",
    "        title=\"Zero-Shot Frame Reconstruction Metrics\",\n",
    "        save_path=\"plots/2.2-frame-metric-bars.png\",\n",
    "        scales=[100.0, 100.0],\n",
    "        ylabel=\"Score (%)\",\n",
    "        ylim=(0, 100),\n",
    "    )\n",
    "    PlotUtils.plot_metrics_heatmap(\n",
    "        frame_results,\n",
    "        \"Zero-Shot Frame Reconstruction Heatmap\",\n",
    "        \"plots/2.2-frame-heatmap.png\",\n",
    "        metrics=[\"strict_match_accuracy\", \"reconstruction_accuracy\"],\n",
    "        metric_labels=[\"Strict Match Accuracy (%)\", \"Reconstruction Accuracy (%)\"],\n",
    "        scales=[100.0, 100.0],\n",
    "    )\n",
    "\n",
    "frame_results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fbfbfb4",
   "metadata": {},
   "source": [
    "2.3 evaluate both models using history-only inputs for action recognition: h → predict y, save to 2.3-result.json\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09895268",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "action_results_path = Path(\"2.3-result.json\")\n",
    "action_raw_path = Path(\"2.3-raw.json\")\n",
    "\n",
    "if action_results_path.exists() and action_raw_path.exists():\n",
    "    action_results = Utils.load_json(action_results_path) or {}\n",
    "    action_raw_outputs = Utils.load_json(action_raw_path) or {}\n",
    "    print(f\"Loaded cached action recognition results from {action_results_path}\")\n",
    "    print(f\"Loaded cached raw outputs from {action_raw_path}\")\n",
    "else:\n",
    "    test_indices = INFERENCE_TEST_INDICES if 'INFERENCE_TEST_INDICES' in globals() else SPLITS['test']\n",
    "\n",
    "    print(\n",
    "        f\"Evaluating action recognition on {len(test_indices)} samples using history length {full_dataset.history_length}.\"\n",
    "    )\n",
    "\n",
    "    action_results = {}\n",
    "    action_raw_outputs = {}\n",
    "    for model_key in MODEL_PATHS:\n",
    "        wrapper = get_model_wrapper(model_key)\n",
    "        metrics = wrapper.evaluate_task(\n",
    "            full_dataset,\n",
    "            test_indices,\n",
    "            task_type=\"action_recognition\",\n",
    "            model_key=model_key,\n",
    "            batch_size=1,\n",
    "            max_new_tokens=MAX_NEW_TOKENS,\n",
    "            action_embedder=ACTION_EMBEDDER,\n",
    "        )\n",
    "\n",
    "        predictions = metrics.get(\"predictions\", [])\n",
    "        targets = metrics.get(\"targets\", [])\n",
    "        raw_records = []\n",
    "        if len(predictions) != len(test_indices):\n",
    "            print(\n",
    "                f\"Warning: prediction count {len(predictions)} does not match test indices {len(test_indices)} for {model_key}.\"\n",
    "            )\n",
    "        for idx, pred, target in zip(test_indices, predictions, targets):\n",
    "            pair = full_dataset.data_pairs[int(idx)]\n",
    "            history = pair.get(\"history_action\") or pair.get(\"history_reconstruction\") or pair.get(\"x\")\n",
    "            raw_records.append(\n",
    "                {\n",
    "                    \"index\": int(idx),\n",
    "                    \"episode\": pair.get(\"episode\"),\n",
    "                    \"history\": history,\n",
    "                    \"y_label\": target,\n",
    "                    \"y_prediction\": pred,\n",
    "                }\n",
    "            )\n",
    "        action_raw_outputs[model_key] = raw_records\n",
    "\n",
    "        action_results[model_key] = {k: v for k, v in metrics.items() if k not in {\"predictions\", \"targets\"}}\n",
    "        release_model(model_key)\n",
    "\n",
    "    Utils.save_json(action_results, action_results_path)\n",
    "    Utils.save_json(action_raw_outputs, action_raw_path)\n",
    "    print(f\"Saved zero-shot action recognition summary to {action_results_path}\")\n",
    "    print(f\"Saved action recognition raw outputs to {action_raw_path}\")\n",
    "\n",
    "    release_all_models()\n",
    "\n",
    "action_results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86ca3c70",
   "metadata": {},
   "source": [
    "2.4 update zero-shot action recognition plots (h → y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80857f24",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "plot_utils = PlotUtils()\n",
    "action_results = Utils.load_json(\"2.3-result.json\") or {}\n",
    "\n",
    "if not action_results:\n",
    "    print(\"No zero-shot action recognition results found. Run cell 2.3 first.\")\n",
    "else:\n",
    "    PlotUtils.plot_multi_metric_bar(\n",
    "        action_results,\n",
    "        metric_keys=[\"strict_match_accuracy\", \"word2vec_cosine\", \"f1\"],\n",
    "        metric_labels=[\"Strict Match Accuracy\", \"Word2Vec Cosine\", \"Macro F1\"],\n",
    "        title=\"Zero-Shot Action Recognition Metrics\",\n",
    "        save_path=\"plots/2.4-action-metric-bars.png\",\n",
    "        scales=[100.0, 100.0, 100.0],\n",
    "        ylabel=\"Score (%)\",\n",
    "        ylim=(0, 100),\n",
    "    )\n",
    "    PlotUtils.plot_metrics_heatmap(\n",
    "        action_results,\n",
    "        \"Zero-Shot Action Recognition Heatmap\",\n",
    "        \"plots/2.4-action-heatmap.png\",\n",
    "        metrics=[\"strict_match_accuracy\", \"word2vec_cosine\", \"precision\", \"recall\", \"f1\"],\n",
    "        metric_labels=[\n",
    "            \"Strict Match Accuracy (%)\",\n",
    "            \"Word2Vec Cosine (%)\",\n",
    "            \"Precision (%)\",\n",
    "            \"Recall (%)\",\n",
    "            \"Macro F1 (%)\",\n",
    "        ],\n",
    "        scales=[100.0, 100.0, 100.0, 100.0, 100.0],\n",
    "    )\n",
    "    for model_key, metrics in action_results.items():\n",
    "        conf = metrics.get(\"confusion_matrix\")\n",
    "        labels = metrics.get(\"labels\", [])\n",
    "        if conf and labels:\n",
    "            PlotUtils.plot_confusion_matrix(\n",
    "                np.array(conf),\n",
    "                labels,\n",
    "                f\"Action Recognition Confusion Matrix ({model_key})\",\n",
    "                f\"plots/2.4-confusion-{model_key}.png\",\n",
    "            )\n",
    "\n",
    "action_results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f26871a2",
   "metadata": {},
   "source": [
    "3.1 fine tune both model with lora method, task is next frame reconstraction, input x and y, output z, with loaded data train and stop in val and monitor via w&b, do not pass model parameter to w&b, keep them in local dir checkpoints with peroper naming and also keep a log in the dir logs. and create 3.1-training-metadata.json file in the working dir given the match betwen the run folder path under checkpoints and the run log path."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "246c5b56",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "training_config = config.get_config()\n",
    "training_config[\"batch_size\"] = max(1, min(training_config[\"batch_size\"], len(SPLITS[\"train\"])))\n",
    "lora_config = {\n",
    "    \"r\": training_config[\"lora_r\"],\n",
    "    \"lora_alpha\": training_config[\"lora_alpha\"],\n",
    "    \"lora_dropout\": training_config[\"lora_dropout\"],\n",
    "}\n",
    "\n",
    "ENABLE_TRAINING = False\n",
    "TRAINING_MODELS = [\"qwen3-0.6b\", \"qwen3-4b\"]\n",
    "\n",
    "training_metadata = {}\n",
    "\n",
    "if ENABLE_TRAINING and SPLITS[\"train\"]:\n",
    "    for model_key in TRAINING_MODELS:\n",
    "        print(f\"Starting LoRA fine-tuning for {model_key}...\")\n",
    "        wrapper = get_model_wrapper(model_key, use_lora=True, lora_config=lora_config, force_reload=True)\n",
    "\n",
    "        train_loader = build_dataloader(\n",
    "            full_dataset,\n",
    "            SPLITS[\"train\"],\n",
    "            wrapper.tokenizer,\n",
    "            \"frame_reconstruction\",\n",
    "            batch_size=training_config[\"batch_size\"],\n",
    "            shuffle=True,\n",
    "            context_examples=None,\n",
    "        )\n",
    "        val_loader = build_dataloader(\n",
    "            full_dataset,\n",
    "            SPLITS[\"val\"],\n",
    "            wrapper.tokenizer,\n",
    "            \"frame_reconstruction\",\n",
    "            batch_size=1,\n",
    "            shuffle=False,\n",
    "            context_examples=None,\n",
    "        )\n",
    "\n",
    "        metadata = wrapper.train(\n",
    "            train_loader,\n",
    "            val_loader,\n",
    "            training_config,\n",
    "            task_name=f\"frame_reconstruction_{model_key}\",\n",
    "            use_wandb=WANDB_ENABLED,\n",
    "        )\n",
    "\n",
    "        metadata_path = Path(f\"3.1-training-metadata-{model_key}.json\")\n",
    "        Utils.save_json(metadata, metadata_path)\n",
    "        training_metadata[model_key] = metadata\n",
    "        release_model(model_key)\n",
    "        del wrapper\n",
    "else:\n",
    "    print(\"Supervised LoRA training skipped. Set ENABLE_TRAINING = True to run fine-tuning.\")\n",
    "\n",
    "release_all_models()\n",
    "\n",
    "training_metadata\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f1bd6cc",
   "metadata": {},
   "source": [
    "3.2 evaluate on test dataset, and save to 3.2-result.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fc5ae36",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "fine_tuned_results_path = Path(\"3.2-result.json\")\n",
    "fine_tuned_raw_path = Path(\"3.2-raw.json\")\n",
    "test_indices = INFERENCE_TEST_INDICES if 'INFERENCE_TEST_INDICES' in globals() else SPLITS['test']\n",
    "\n",
    "if fine_tuned_results_path.exists() and fine_tuned_raw_path.exists():\n",
    "    fine_tuned_results = Utils.load_json(fine_tuned_results_path) or {}\n",
    "    fine_tuned_raw_outputs = Utils.load_json(fine_tuned_raw_path) or {}\n",
    "    print(f\"Loaded cached fine-tuned frame reconstruction results from {fine_tuned_results_path}\")\n",
    "    print(f\"Loaded cached raw outputs from {fine_tuned_raw_path}\")\n",
    "else:\n",
    "    fine_tuned_results = {}\n",
    "    fine_tuned_raw_outputs = {}\n",
    "    for model_key in TRAINING_MODELS:\n",
    "        metadata_path = Path(f\"3.1-training-metadata-{model_key}.json\")\n",
    "        metadata = Utils.load_json(metadata_path)\n",
    "        if not metadata:\n",
    "            print(f\"No training metadata found for {model_key}; skipping.\")\n",
    "            continue\n",
    "\n",
    "        wrapper = get_model_wrapper(model_key, use_lora=True, lora_config=lora_config, force_reload=True)\n",
    "        checkpoint_dir = Path(metadata[\"checkpoint_dir\"])\n",
    "        adapter_path = checkpoint_dir / \"best_lora_adapter\"\n",
    "        model_path = checkpoint_dir / \"best_model.pt\"\n",
    "\n",
    "        if adapter_path.exists():\n",
    "            wrapper.load_checkpoint(str(adapter_path))\n",
    "        elif model_path.exists():\n",
    "            wrapper.load_checkpoint(str(model_path))\n",
    "        else:\n",
    "            print(f\"No fine-tuned weights found for {model_key}; skipping evaluation.\")\n",
    "            release_model(model_key)\n",
    "            del wrapper\n",
    "            continue\n",
    "\n",
    "        metrics = evaluate_wrapper(\n",
    "            wrapper,\n",
    "            model_key,\n",
    "            \"frame_reconstruction\",\n",
    "            test_indices,\n",
    "            context_examples=None,\n",
    "        )\n",
    "\n",
    "        predictions = metrics.get(\"predictions\", [])\n",
    "        targets = metrics.get(\"targets\", [])\n",
    "        raw_records = []\n",
    "        if len(predictions) != len(test_indices):\n",
    "            print(\n",
    "                f\"Warning: prediction count {len(predictions)} does not match test indices {len(test_indices)} for {model_key}.\"\n",
    "            )\n",
    "        for idx, pred, target in zip(test_indices, predictions, targets):\n",
    "            pair = full_dataset.data_pairs[int(idx)]\n",
    "            history = pair.get(\"history_reconstruction\") or pair.get(\"history_action\") or pair.get(\"x\")\n",
    "            raw_records.append(\n",
    "                {\n",
    "                    \"index\": int(idx),\n",
    "                    \"episode\": pair.get(\"episode\"),\n",
    "                    \"history\": history,\n",
    "                    \"z_label\": target,\n",
    "                    \"z_prediction\": pred,\n",
    "                }\n",
    "            )\n",
    "        fine_tuned_raw_outputs[model_key] = raw_records\n",
    "\n",
    "        fine_tuned_results[model_key] = {k: v for k, v in metrics.items() if k not in {\"predictions\", \"targets\"}}\n",
    "        release_model(model_key)\n",
    "        del wrapper\n",
    "\n",
    "    if fine_tuned_results:\n",
    "        Utils.save_json(fine_tuned_results, fine_tuned_results_path)\n",
    "        Utils.save_json(fine_tuned_raw_outputs, fine_tuned_raw_path)\n",
    "        print(f\"Saved fine-tuned evaluation results to {fine_tuned_results_path}\")\n",
    "        print(f\"Saved fine-tuned raw outputs to {fine_tuned_raw_path}\")\n",
    "    else:\n",
    "        print(\"No fine-tuned results to save.\")\n",
    "\n",
    "    release_all_models()\n",
    "\n",
    "fine_tuned_results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e86f2a8e",
   "metadata": {},
   "source": [
    "3.3 plot the evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7398c62",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "plot_utils = PlotUtils()\n",
    "\n",
    "zero_shot_frame = Utils.load_json(\"2.1-result.json\") or {}\n",
    "fine_tuned_frame = Utils.load_json(\"3.2-result.json\") or {}\n",
    "\n",
    "method_results = {}\n",
    "if zero_shot_frame:\n",
    "    method_results[\"zero_shot\"] = zero_shot_frame\n",
    "if fine_tuned_frame:\n",
    "    method_results[\"fine_tuned\"] = fine_tuned_frame\n",
    "\n",
    "if len(method_results) >= 2:\n",
    "    PlotUtils.plot_method_metric_bar(\n",
    "        method_results,\n",
    "        metric_key=\"reconstruction_accuracy\",\n",
    "        title=\"Frame Reconstruction: Reconstruction Accuracy Comparison\",\n",
    "        save_path=\"plots/3.3-frame-reconstruction-accuracy.png\",\n",
    "        scale=100.0,\n",
    "        ylabel=\"Reconstruction Accuracy (%)\",\n",
    "        method_labels={\"zero_shot\": \"Zero-Shot\", \"fine_tuned\": \"LoRA Fine-Tuned\"},\n",
    "        metric_label=\"Reconstruction Accuracy (%)\",\n",
    "        ylim=(0, 100),\n",
    "    )\n",
    "    PlotUtils.plot_method_metric_bar(\n",
    "        method_results,\n",
    "        metric_key=\"strict_match_accuracy\",\n",
    "        title=\"Frame Reconstruction: Strict Match Accuracy Comparison\",\n",
    "        save_path=\"plots/3.3-frame-strict-accuracy.png\",\n",
    "        scale=100.0,\n",
    "        ylabel=\"Strict Match Accuracy (%)\",\n",
    "        method_labels={\"zero_shot\": \"Zero-Shot\", \"fine_tuned\": \"LoRA Fine-Tuned\"},\n",
    "        metric_label=\"Strict Match Accuracy (%)\",\n",
    "        ylim=(0, 100),\n",
    "    )\n",
    "else:\n",
    "    print(\"Need results from at least two methods to plot comparisons. Run zero-shot (2.1) and fine-tuned (3.2) evaluations.\")\n",
    "\n",
    "{\"zero_shot\": zero_shot_frame, \"fine_tuned\": fine_tuned_frame}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dba795fd",
   "metadata": {},
   "source": [
    "### 4.1 LoRA Fine-Tuning for Action Recognition\n",
    "\n",
    "Fine-tune both Qwen models on the action recognition task using LoRA. Each run stores checkpoints in `checkpoints/`, logs in `logs/`, and records training metadata to `4.1-training-metadata.json` for downstream evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbf6e959",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "action_training_config = config.get_config()\n",
    "action_training_config[\"batch_size\"] = max(1, min(action_training_config[\"batch_size\"], len(SPLITS[\"train\"])))\n",
    "action_lora_config = {\n",
    "    \"r\": action_training_config[\"lora_r\"],\n",
    "    \"lora_alpha\": action_training_config[\"lora_alpha\"],\n",
    "    \"lora_dropout\": action_training_config[\"lora_dropout\"],\n",
    "}\n",
    "\n",
    "ENABLE_ACTION_TRAINING = True\n",
    "ACTION_TRAINING_MODELS = [\"qwen3-0.6b\", \"qwen3-4b\"]\n",
    "\n",
    "action_training_metadata = {}\n",
    "\n",
    "if not SPLITS[\"train\"]:\n",
    "    print(\"No training samples available for action recognition. Populate SPLITS['train'] before running fine-tuning.\")\n",
    "elif ENABLE_ACTION_TRAINING:\n",
    "    for model_key in ACTION_TRAINING_MODELS:\n",
    "        print(f\"Starting LoRA action recognition fine-tuning for {model_key}...\")\n",
    "        wrapper = get_model_wrapper(model_key, use_lora=True, lora_config=action_lora_config, force_reload=True)\n",
    "\n",
    "        train_loader = build_dataloader(\n",
    "            full_dataset,\n",
    "            SPLITS[\"train\"],\n",
    "            wrapper.tokenizer,\n",
    "            \"action_recognition\",\n",
    "            batch_size=action_training_config[\"batch_size\"],\n",
    "            shuffle=True,\n",
    "            context_examples=None,\n",
    "        )\n",
    "        val_loader = build_dataloader(\n",
    "            full_dataset,\n",
    "            SPLITS[\"val\"],\n",
    "            wrapper.tokenizer,\n",
    "            \"action_recognition\",\n",
    "            batch_size=1,\n",
    "            shuffle=False,\n",
    "            context_examples=None,\n",
    "        )\n",
    "\n",
    "        metadata = wrapper.train(\n",
    "            train_loader,\n",
    "            val_loader,\n",
    "            action_training_config,\n",
    "            task_name=f\"action_recognition_{model_key}\",\n",
    "            use_wandb=WANDB_ENABLED,\n",
    "        )\n",
    "        metadata[\"task_type\"] = \"action_recognition\"\n",
    "        metadata[\"config\"] = {\n",
    "            key: action_training_config[key]\n",
    "            for key in [\n",
    "                \"learning_rate\",\n",
    "                \"num_epochs\",\n",
    "                \"batch_size\",\n",
    "                \"lora_r\",\n",
    "                \"lora_alpha\",\n",
    "                \"lora_dropout\",\n",
    "                \"warmup_steps\",\n",
    "                \"max_grad_norm\",\n",
    "            ]\n",
    "            if key in action_training_config\n",
    "        }\n",
    "\n",
    "        metadata_path = Path(f\"4.1-training-metadata-{model_key}.json\")\n",
    "        Utils.save_json(metadata, metadata_path)\n",
    "        action_training_metadata[model_key] = metadata\n",
    "        release_model(model_key)\n",
    "        del wrapper\n",
    "\n",
    "    Utils.save_json(action_training_metadata, \"4.1-training-metadata.json\")\n",
    "    print(\"Saved aggregated training metadata to 4.1-training-metadata.json\")\n",
    "else:\n",
    "    print(\"Action recognition LoRA training skipped. Set ENABLE_ACTION_TRAINING = True to run fine-tuning.\")\n",
    "\n",
    "release_all_models()\n",
    "\n",
    "action_training_metadata\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b1b8288",
   "metadata": {},
   "source": [
    "### 4.2 Evaluate Fine-Tuned Action Recognition\n",
    "\n",
    "Load the best adapters from Section 4.1, run inference on the test split, and persist aggregated metrics to `4.2-result.json`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14cc34b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "action_finetuned_results_path = Path(\"4.2-result.json\")\n",
    "action_finetuned_raw_path = Path(\"4.2-raw.json\")\n",
    "test_indices = INFERENCE_TEST_INDICES if 'INFERENCE_TEST_INDICES' in globals() else SPLITS['test']\n",
    "\n",
    "if action_finetuned_results_path.exists() and action_finetuned_raw_path.exists():\n",
    "    action_finetuned_results = Utils.load_json(action_finetuned_results_path) or {}\n",
    "    action_finetuned_raw_outputs = Utils.load_json(action_finetuned_raw_path) or {}\n",
    "    print(f\"Loaded cached fine-tuned action recognition results from {action_finetuned_results_path}\")\n",
    "    print(f\"Loaded cached raw outputs from {action_finetuned_raw_path}\")\n",
    "else:\n",
    "    action_finetuned_results = {}\n",
    "    action_finetuned_raw_outputs = {}\n",
    "    metadata_index = Utils.load_json(\"4.1-training-metadata.json\") or {}\n",
    "\n",
    "    if not metadata_index:\n",
    "        print(\"No action recognition training metadata found. Run cell 4.1 first.\")\n",
    "    elif not test_indices:\n",
    "        print(\"No test samples available for action recognition evaluation. Populate test indices before running evaluation.\")\n",
    "    else:\n",
    "        for model_key, metadata in metadata_index.items():\n",
    "            checkpoint_dir = Path(metadata.get(\"checkpoint_dir\", \"\"))\n",
    "            if not checkpoint_dir.exists():\n",
    "                print(f\"Checkpoint directory {checkpoint_dir} not found for {model_key}; skipping.\")\n",
    "                continue\n",
    "\n",
    "            wrapper = get_model_wrapper(model_key, use_lora=True, lora_config=action_lora_config, force_reload=True)\n",
    "\n",
    "            adapter_path = checkpoint_dir / \"best_lora_adapter\"\n",
    "            model_path = checkpoint_dir / \"best_model.pt\"\n",
    "\n",
    "            if adapter_path.exists():\n",
    "                wrapper.load_checkpoint(str(adapter_path))\n",
    "            elif model_path.exists():\n",
    "                wrapper.load_checkpoint(str(model_path))\n",
    "            else:\n",
    "                print(f\"No fine-tuned weights found for {model_key}; skipping evaluation.\")\n",
    "                release_model(model_key)\n",
    "                del wrapper\n",
    "                continue\n",
    "\n",
    "            metrics = evaluate_wrapper(\n",
    "                wrapper,\n",
    "                model_key,\n",
    "                \"action_recognition\",\n",
    "                test_indices,\n",
    "                context_examples=None,\n",
    "            )\n",
    "\n",
    "            predictions = metrics.get(\"predictions\", [])\n",
    "            targets = metrics.get(\"targets\", [])\n",
    "            raw_records = []\n",
    "            if len(predictions) != len(test_indices):\n",
    "                print(\n",
    "                    f\"Warning: prediction count {len(predictions)} does not match test indices {len(test_indices)} for {model_key}.\"\n",
    "                )\n",
    "            for idx, pred, target in zip(test_indices, predictions, targets):\n",
    "                pair = full_dataset.data_pairs[int(idx)]\n",
    "                history = pair.get(\"history_action\") or pair.get(\"history_reconstruction\") or pair.get(\"x\")\n",
    "                raw_records.append(\n",
    "                    {\n",
    "                        \"index\": int(idx),\n",
    "                        \"episode\": pair.get(\"episode\"),\n",
    "                        \"history\": history,\n",
    "                        \"y_label\": target,\n",
    "                        \"y_prediction\": pred,\n",
    "                    }\n",
    "                )\n",
    "            action_finetuned_raw_outputs[model_key] = raw_records\n",
    "\n",
    "            action_finetuned_results[model_key] = {\n",
    "                k: v for k, v in metrics.items() if k not in {\"predictions\", \"targets\"}\n",
    "            }\n",
    "            release_model(model_key)\n",
    "            del wrapper\n",
    "\n",
    "    if action_finetuned_results:\n",
    "        Utils.save_json(action_finetuned_results, action_finetuned_results_path)\n",
    "        Utils.save_json(action_finetuned_raw_outputs, action_finetuned_raw_path)\n",
    "        print(f\"Saved action recognition fine-tuned evaluation results to {action_finetuned_results_path}\")\n",
    "        print(f\"Saved action recognition fine-tuned raw outputs to {action_finetuned_raw_path}\")\n",
    "    elif not (action_finetuned_results_path.exists() and action_finetuned_raw_path.exists()):\n",
    "        print(\"No fine-tuned action recognition results to save.\")\n",
    "\n",
    "    release_all_models()\n",
    "\n",
    "action_finetuned_results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84502e15",
   "metadata": {},
   "source": [
    "### 4.3 Plot Action Recognition Comparison\n",
    "\n",
    "Compare zero-shot and LoRA fine-tuned performance using bar charts saved under `plots/`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fce88003",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_utils = PlotUtils()\n",
    "\n",
    "zero_shot_action = Utils.load_json(\"2.3-result.json\") or {}\n",
    "fine_tuned_action = Utils.load_json(\"4.2-result.json\") or {}\n",
    "\n",
    "method_results = {}\n",
    "if zero_shot_action:\n",
    "    method_results[\"zero_shot\"] = zero_shot_action\n",
    "if fine_tuned_action:\n",
    "    method_results[\"fine_tuned\"] = fine_tuned_action\n",
    "\n",
    "if len(method_results) >= 2:\n",
    "    PlotUtils.plot_method_metric_bar(\n",
    "        method_results,\n",
    "        metric_key=\"strict_match_accuracy\",\n",
    "        title=\"Action Recognition: Strict Match Accuracy Comparison\",\n",
    "        save_path=\"plots/4.3-action-strict-accuracy.png\",\n",
    "        scale=100.0,\n",
    "        ylabel=\"Strict Match Accuracy (%)\",\n",
    "        method_labels={\"zero_shot\": \"Zero-Shot\", \"fine_tuned\": \"LoRA Fine-Tuned\"},\n",
    "        metric_label=\"Strict Match Accuracy (%)\",\n",
    "        ylim=(0, 100),\n",
    "    )\n",
    "    PlotUtils.plot_method_metric_bar(\n",
    "        method_results,\n",
    "        metric_key=\"f1\",\n",
    "        title=\"Action Recognition: Macro F1 Comparison\",\n",
    "        save_path=\"plots/4.3-action-macro-f1.png\",\n",
    "        scale=100.0,\n",
    "        ylabel=\"Macro F1 (%)\",\n",
    "        method_labels={\"zero_shot\": \"Zero-Shot\", \"fine_tuned\": \"LoRA Fine-Tuned\"},\n",
    "        metric_label=\"Macro F1 (%)\",\n",
    "        ylim=(0, 100),\n",
    "    )\n",
    "else:\n",
    "    print(\"Need zero-shot and fine-tuned results to plot comparisons. Run cells 2.3 and 4.2.\")\n",
    "\n",
    "{\"zero_shot\": zero_shot_action, \"fine_tuned\": fine_tuned_action}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
